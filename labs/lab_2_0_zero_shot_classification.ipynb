{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AI Workshop - Lab 2-0: Exploring Zero-Shot Image Classification\n",
    "\n",
    "In this lab, you'll explore the capabilities of **zero-shot image classification** using a pre-trained model. This method allows you to classify images into categories **without additional training**—you simply provide the model with a list of candidate labels.\n",
    "\n",
    "The goal of this lab is to:\n",
    "1. Experiment with different levels of classification (e.g., coarse, medium, and fine-grained categories).\n",
    "2. Design your own classification use cases by selecting or capturing images and thinking critically about meaningful labels.\n",
    "3. Understand real-world applications of zero-shot image classification.\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Do\n",
    "1. **Learn to use the `transformers` pipeline** for zero-shot image classification.\n",
    "2. **Test the model** on provided examples with coarse, medium, and fine-grained categories.\n",
    "3. **Choose your own images** (either by URL or uploading files) and create custom lists of candidate labels.\n",
    "4. **Reflect on use cases** for this technology in real-world scenarios.\n",
    "\n",
    "### Why CLIP?\n",
    "OpenAI’s CLIP (Contrastive Language–Image Pretraining) is a state-of-the-art model trained on a large dataset of image-text pairs. It:\n",
    "- Understands relationships between images and text.\n",
    "- Performs well across many tasks without fine-tuning.\n",
    "- Enables customizable and versatile image classification with minimal effort.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Loading the Zero-Shot Classification Pipeline\n",
    "\n",
    "We’ll use the transformers library to load a zero-shot image classification pipeline with a pre-trained model. This pipeline can classify images into user-defined categories. Run the following code to initialize the pipeline:\n"
   ],
   "id": "15362f9c775d24af"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the zero-shot image classification pipeline\n",
    "image_classifier = pipeline(task=\"zero-shot-image-classification\", model=\"openai/clip-vit-large-patch14\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Step 2: Try an Example Image\n",
    "\n",
    "Start with a sample image from an online dataset: a photo of a cat. We’ll classify this image into two categories: \"animal\" and \"vehicle\". Run the code below to see the model’s predictions:"
   ],
   "id": "153474a90a494ef5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg/1920px-Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg\"\n",
    "\n",
    "# Display the image\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ],
   "id": "d1ef1c19a96590a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let’s classify the image using the zero-shot pipeline:",
   "id": "dcdee21fc158e746"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categories = [\"animal\", \"vehicle\"]\n",
    "\n",
    "# Classify the image\n",
    "outputs = image_classifier(image, candidate_labels=categories)\n",
    "outputs = [{\"score\": round(output[\"score\"], 4), \"label\": output[\"label\"] } for output in outputs]\n",
    "outputs"
   ],
   "id": "720433197d53cec7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The model predicts the label \"animal\" with a higher confidence score than \"vehicle\". This is expected, as the image contains a cat.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Label granularity\n",
    "\n",
    "Next, let’s experiment with different levels of label granularity. We'll try classifying the image into more specific sets of categories. For convenience, we'll also write a function that takes a URL and a list of labels, and displays the image along with the model's predictions."
   ],
   "id": "7453e638ec9b669"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def classify_image(image_url, categories):\n",
    "    # Display the image\n",
    "    response = requests.get(image_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Classify the image\n",
    "    outputs = image_classifier(image_url, candidate_labels=categories)\n",
    "    outputs = [{\"score\": round(output[\"score\"], 4), \"label\": output[\"label\"] } for output in outputs]\n",
    "    return outputs\n",
    "\n",
    "# Coarse-grained categories\n",
    "coarse_labels = [\"animal\", \"vehicle\", \"appliance\"]\n",
    "classify_image(image_url, coarse_labels)"
   ],
   "id": "fa6f521781bb43e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "medium_labels = [\"cat\", \"dog\", \"car\", \"airplane\"]\n",
    "classify_image(image_url, medium_labels)"
   ],
   "id": "67d89f7c60d434dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fine_labels = [\"tabby cat\", \"Siamese cat\", \"golden retriever\", \"sports car\"]\n",
    "classify_image(image_url, fine_labels)"
   ],
   "id": "2cdd0aa2a1709ea1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Choose Your Own Images and Labels\n",
    "\n",
    "Zero-shot classification is highly useful in a range of industrial and consumer applications. You can use this technique to classify images in real-time without retraining models.\n",
    "\n",
    "#### Your Task:\n",
    "1. **Find or Capture Images**: Search for images online or capture photos of objects around you.\n",
    "2. **Define Candidate Labels**: Create a list of candidate labels that describe the images.\n",
    "3. **Classify the Images**: Use the zero-shot pipeline to classify the images based on your labels.\n",
    "\n",
    "Consider how this technology could be applied in scenarios like:\n",
    "\n",
    "- **Retail**: Automatically categorize products based on images.\n",
    "- **Healthcare**: Identify medical conditions from diagnostic images.\n",
    "- **Manufacturing**: Inspect products for defects on the assembly line.\n",
    "\n",
    "Try expanding the task beyond describing objects. For example, you could classify images based on emotions, actions, or abstract concepts.\n",
    "\n",
    "As a bonus, you can also try using Google's SigLIP model, a competitor to OpenAI's CLIP, by changing the model name in the pipeline initialization. To do so replace `model=openai/clip-vit-large-patch14` with `model=google/siglip-so400m-patch14-384` in the pipeline initialization code at the start of the notebook."
   ],
   "id": "151ca630961727f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example: Classifying a custom image\n",
    "custom_image_url = \"https://example.com/your-image.jpg\"\n",
    "custom_labels = [\"label1\", \"label2\", \"label3\"]\n",
    "\n",
    "classify_image(custom_image_url, custom_labels)"
   ],
   "id": "2da925bd179441c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### How Zero-Shot Learning Models Are Trained\n",
    "\n",
    "Zero-shot learning models are designed to handle tasks they weren’t specifically trained for by using general patterns learned from large datasets. Here’s how they are trained and why they work:\n",
    "\n",
    "#### 1. **Pre-training on Large Datasets**\n",
    "Zero-shot models are trained on huge datasets containing pairs of:\n",
    "- **Images**: Photos, illustrations, or other visual content.\n",
    "- **Text Descriptions**: Sentences, captions, or labels describing what’s in the image.\n",
    "\n",
    "For example, an image of a dog might be paired with the text “a golden retriever running in a park.” By seeing millions of these pairs, the model learns how visual features (like shapes and colors) relate to text descriptions.\n",
    "\n",
    "#### 2. **Building a Shared Embedding Space**\n",
    "The model has two main parts:\n",
    "- **Image Encoder**: Converts images into numerical representations (vectors) that summarize their features.\n",
    "- **Text Encoder**: Converts text into similar numerical representations.\n",
    "\n",
    "Both encoders learn to map images and text into a common \"embedding space,\" where:\n",
    "- Similar images and text descriptions are placed close together.\n",
    "- Unrelated images and text are placed far apart.\n",
    "\n",
    "#### 3. **Generalized Learning**\n",
    "Instead of being trained for specific tasks (like \"dog vs. cat classification\"), the model learns general features about:\n",
    "- How images and text relate to each other.\n",
    "- Concepts that appear across many tasks (e.g., animals, vehicles, actions).\n",
    "\n",
    "Because of this generalization, the model can handle new tasks it wasn’t explicitly trained on, as long as you provide it with meaningful candidate labels.\n",
    "\n",
    "#### 4. **How It Works at Inference Time**\n",
    "When you give the model a new image and candidate labels (e.g., \"dog,\" \"cat,\" \"car\"):\n",
    "1. The **image encoder** processes the image and creates a vector representing its features.\n",
    "2. The **text encoder** processes each candidate label and converts it into a vector.\n",
    "3. The model compares the image vector to each label vector, measuring how \"close\" they are in the embedding space.\n",
    "4. Labels with higher similarity scores are more likely to match the image.\n",
    "\n",
    "#### Why Zero-Shot Learning is Useful\n",
    "- **No Task-Specific Training**: You don’t need to train the model on every task you want to solve.\n",
    "- **Flexible**: You can easily change the categories (labels) to fit a new task.\n",
    "- **Scalable**: It works for tasks where labeled data is hard to get or doesn’t exist.\n",
    "\n",
    "#### Example Applications\n",
    "- **Content Tagging**: Automatically categorize photos or videos into broad or specific labels.\n",
    "- **Accessibility**: Provide descriptions for images to assist visually impaired users.\n",
    "- **Real-Time Analysis**: Quickly classify images in dynamic environments (e.g., traffic cameras).\n",
    "\n",
    "By leveraging patterns learned from diverse data, zero-shot models can handle tasks with minimal setup, making them incredibly versatile for real-world use cases."
   ],
   "id": "d1d4f551921b4b53"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
