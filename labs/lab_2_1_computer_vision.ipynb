{
 "cells": [
  {
   "metadata": {
    "id": "858d6cd9fd7f48a7"
   },
   "cell_type": "markdown",
   "source": [
    "# AI Workshop - Lab 2-1: Computer Vision\n",
    "\n",
    "In this lab, we will use pre-trained models to classify machine part defects. Like yesterday, we will be working with Keras and TensorFlow, but this time we will use the `tf.keras.applications` module to load pre-trained models.\n",
    "\n",
    "### Data Overview\n",
    "\n",
    "The dataset provided for this lab contains images of an automotive part, the **Fender Apron**, captured under varying conditions such as different angles and scales. The dataset has already been labeled as either **defective** or **healthy**, making it ideal for supervised learning tasks.\n",
    "\n",
    "- **Total Images**: 250\n",
    "  - **Healthy Parts**: 139 images\n",
    "  - **Defective Parts**: 111 images\n",
    "- **Train/Test Split**:\n",
    "  - **Training Set**: 90% of the data\n",
    "  - **Test Set**: 10% of the data (25 randomly selected images)\n",
    "\n",
    "### Key Steps in Lab\n",
    "1. **Exploration**: We'll start by visualizing the dataset, inspecting some sample images to understand variations and potential challenges.\n",
    "2. **Preprocessing**: Learn to preprocess images by resizing, normalizing pixel values, and augmenting the dataset to simulate real-world scenarios.\n",
    "3. **Model Selection**:\n",
    "   - We'll use the MobileNetV2 architecture, a lightweight and efficient model available in `tf.keras.applications`.\n",
    "   - The pre-trained model will be fine-tuned to classify images into **defective** and **healthy** categories.\n",
    "4. **Evaluation**:\n",
    "   - Evaluate model performance using metrics such as **f1-score**, **precision**, and **recall**.\n",
    "   - Analyze confusion matrices and visualize predictions for better interpretability.\n",
    "\n",
    "### Goals\n",
    "By the end of this lab, you will:\n",
    "- Understand the concept of transfer learning and how to adapt a pre-trained model to solve specific problems.\n",
    "- Gain hands-on experience with training and deploying a defect detection system.\n",
    "\n",
    "Now, let's dive into the **dataset preparation** and explore how these images can be preprocessed for model training!\n",
    "\n",
    "### Loading the Dataset\n",
    "\n",
    "We will start by loading the dataset and visualizing a few sample images to understand the data distribution and characteristics. The dataset is already organized for you into training and testing sets, with separate folders for **defective** and **healthy** parts. Keras makes it easy for us to load images like this using the `image_dataset_from_directory` function.\n",
    "\n",
    "The following command will download the dataset to your workspace using `wget`, a command-line utility for downloading files from the web. Run the cell below to download the dataset."
   ],
   "id": "858d6cd9fd7f48a7"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bc3ccb0a28f46cb",
    "outputId": "6570a36f-62c9-4f96-8ad4-6f34b03fa9c0"
   },
   "cell_type": "code",
   "source": [
    "!wget https://github.com/alexwolson/mdlw_materials/raw/refs/heads/main/data/parts_dataset.tar.gz"
   ],
   "id": "2bc3ccb0a28f46cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we will extract the dataset using the `tar` command. Run the cell below to extract the dataset.",
   "id": "a0f3f9881e64f181"
  },
  {
   "metadata": {
    "id": "cf4c0b1f9e705c82"
   },
   "cell_type": "code",
   "source": [
    "!tar -xf parts_dataset.tar.gz"
   ],
   "id": "cf4c0b1f9e705c82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this section, we will load our image dataset into TensorFlow for training and evaluation. TensorFlow provides an easy-to-use utility, `image_dataset_from_directory`, to load and preprocess image data directly from a directory structure.\n",
    "\n",
    "#### Steps:\n",
    "1. **Organizing the Dataset**:\n",
    "   - The dataset is stored in a directory called `parts_dataset`, with subfolders for `train` and `test`.\n",
    "   - Each subfolder contains images organized by class (e.g., \"defective\" and \"healthy\").\n",
    "\n",
    "2. **Loading the Data**:\n",
    "   - We use `image_dataset_from_directory` to load images from the `train` and `test` directories.\n",
    "   - Images are resized to **224x224 pixels** (a common size for models like MobileNetV2).\n",
    "   - The dataset is divided into **batches of 32 images** for efficient training."
   ],
   "id": "df4420c0f3c7d41b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
    "from keras.layers import Rescaling, RandomCrop, RandomFlip, RandomRotation, RandomZoom, RandomContrast, \\\n",
    "    GlobalAveragePooling2D, Dense, Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing import image, image_dataset_from_directory"
   ],
   "id": "bde9b36e9ae7d5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "initial_id",
    "outputId": "477dd492-2844-4ee7-d4ee-2dc7db49b8b7"
   },
   "source": [
    "# Define the dataset directory\n",
    "dataset_dir = Path('parts_dataset')\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    dataset_dir / 'train',\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    dataset_dir / 'test',\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To better understand our dataset, it’s helpful to visualize some sample images along with their corresponding labels. This can give us insight into the data quality, variations, and any potential preprocessing needs.\n",
    "\n",
    "#### Steps:\n",
    "1. **Take a Batch**:\n",
    "   - Use the `.take(1)` method to extract the first batch of images and labels from the `train_dataset`.\n",
    "2. **Display Images**:\n",
    "   - Use `matplotlib.pyplot` to create a grid of 3x3 images (9 total).\n",
    "   - Convert the image tensors to NumPy arrays and ensure the pixel values are properly scaled for display.\n",
    "3. **Label the Images**:\n",
    "   - Assign a title of \"defective\" or \"healthy\" based on the label value (e.g., `0` for defective and `1` for healthy)."
   ],
   "id": "44e8c54871560787"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "id": "d3e1f93ea2a473e",
    "outputId": "34f20e40-9db8-4f0e-8039-291b4282b8cf"
   },
   "cell_type": "code",
   "source": [
    "# Visualize the first 9 images from the training set\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):                          # Take the first batch from the training dataset\n",
    "    for i in range(9):                                                # Loop through the first 9 images in the batch\n",
    "        ax = plt.subplot(3, 3, i + 1)                                 # Create a 3x3 grid of subplots\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))                 # Display the image\n",
    "        plt.title(\"defective\" if labels[i] == 0 else \"healthy\")       # Add a title based on the label\n",
    "        plt.axis(\"off\")                                               # Remove axis lines for cleaner visualization"
   ],
   "id": "d3e1f93ea2a473e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "302033d4f0b96458"
   },
   "cell_type": "markdown",
   "source": [
    "### Preprocessing the Images\n",
    "\n",
    "When working with image data, preprocessing is a crucial step to prepare the dataset for training a machine learning model. Preprocessing ensures that the data is in the right format, size, and range for optimal performance. Let's review what has already been done for you and what still needs to be completed.\n",
    "\n",
    "#### Preprocessing Already Done:\n",
    "1. **Resizing**:\n",
    "   - The original images (4160 × 3120 pixels) were too large for most models.\n",
    "   - They have been resized to 224 × 224 pixels for training, with padding (black bars) added where necessary to maintain a 1:1 aspect ratio.\n",
    "2. **Train-Test Split**:\n",
    "   - The dataset has been split into training (90%) and testing (10%) sets, so you can focus on model training and evaluation separately.\n",
    "\n",
    "#### Preprocessing to Complete:\n",
    "Now it's your turn to finish preprocessing. Here's what we'll do next:\n",
    "1. **Normalization**:\n",
    "   - Rescale pixel values from their original range ([0, 255]) to the range [0, 1]. This helps models train more effectively by ensuring consistent input scales.\n",
    "2. **Data Augmentation**:\n",
    "   - Apply random transformations (e.g., flips, rotations, zooms) to the training images to simulate variations seen in real-world scenarios. This enhances the model’s ability to generalize.\n",
    "3. **Dataset Configuration**:\n",
    "   - Optimize dataset performance by applying prefetching, caching, and shuffling techniques.\n",
    "\n",
    "#### Viewing the Data:\n",
    "For models, images are represented as matrices of pixel values. Run the code below to inspect the shape of the first image in the training set and view its pixel values (specifically a slice through the middle of the image)."
   ],
   "id": "302033d4f0b96458"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b888b44480d52d78",
    "outputId": "a422f4a2-2aaa-452a-9b3f-d1a7c55ec222"
   },
   "cell_type": "code",
   "source": [
    "for images, labels in train_dataset.take(1):\n",
    "    print(\"Image Shape:\", images[0].shape)            # Shape of the first image in the training set\n",
    "    print(\"Pixel Values (Row 100):\", images[0][100])  # Pixel values in the 100th row\n",
    "    break"
   ],
   "id": "b888b44480d52d78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "bc4b3f8dc043da45"
   },
   "cell_type": "markdown",
   "source": [
    "As you can see from the output above, the pixel values of the images are currently in the range [0, 255]. Each pixel has three values (red, green, and blue), corresponding to the **RGB format** of the image. This format allows the model to process color images, with each color channel represented as a separate value.\n",
    "\n",
    "### Normalizing Pixel Values\n",
    "To prepare the images for training, we need to normalize the pixel values to the range [0, 1]. Normalization ensures that:\n",
    "1. The model trains more effectively by reducing the variability in input values.\n",
    "2. The input values are on a consistent scale, which helps gradient-based optimization algorithms converge faster.\n",
    "\n",
    "We will use Keras's `Rescaling` layer to handle the normalization process. The `Rescaling` layer applies a simple transformation:\n",
    "\n",
    "$$\n",
    "\\text{Normalized Value} = \\text{Original Value} \\times \\frac{1}{255}\n",
    "$$\n",
    "\n",
    "#### Steps:\n",
    "1. Create a normalization layer using `Rescaling(1./255)`, which divides all pixel values by 255.\n",
    "2. Apply the normalization layer to the training dataset using the `map()` function, which processes each image in the dataset.\n",
    "\n",
    "Run the following code to normalize the dataset:"
   ],
   "id": "bc4b3f8dc043da45"
  },
  {
   "metadata": {
    "id": "3f37186fe65647e"
   },
   "cell_type": "code",
   "source": [
    "# Create a normalization layer\n",
    "normalization_layer = Rescaling(1./255)\n",
    "\n",
    "# Apply normalization to the training dataset\n",
    "normalized_train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))"
   ],
   "id": "3f37186fe65647e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba0547e086b81bd7",
    "outputId": "6b49ba22-9d4d-46cd-f6ec-cc7cec0caecd"
   },
   "cell_type": "code",
   "source": [
    "for images, labels in normalized_train_dataset.take(1):\n",
    "    print(\"Image Shape:\", images[0].shape)                       # Shape of the normalized image\n",
    "    print(\"Normalized Pixel Values (Row 100):\", images[0][100])  # Pixel values in the 100th row\n",
    "    break"
   ],
   "id": "ba0547e086b81bd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "91421debc0a1a1dc"
   },
   "cell_type": "markdown",
   "source": [
    "### Data Augmentation: Enhancing Generalization\n",
    "\n",
    "With the pixel values normalized, the next step in preprocessing is **data augmentation**, which helps improve the model's ability to generalize to unseen data. By applying random transformations to the training images, we can:\n",
    "- Simulate variations found in real-world scenarios.\n",
    "- Reduce overfitting by artificially increasing the diversity of the training dataset.\n",
    "\n",
    "Data augmentation helps the model learn **invariance** to changes that don’t matter for the task at hand. For instance:\n",
    "- **Flipping** or **rotating** an image should not affect whether the model identifies a defect.\n",
    "- Variations in **brightness**, **contrast**, or **zoom** should not change the model’s prediction.\n",
    "\n",
    "By applying these random transformations, we teach the model to focus on meaningful patterns in the data, rather than being misled by irrelevant differences. This improves robustness and generalization, especially when the training dataset is small.\n",
    "\n",
    "#### Augmentation in Keras\n",
    "Keras provides convenient **image augmentation layers** that can be added directly to the model architecture or applied as a preprocessing step. These layers make it simple to experiment with different strategies.\n",
    "\n",
    "The augmentation layers we’ll use include:\n",
    "- **RandomCrop**: Crops the image to a specified size.\n",
    "- **RandomFlip**: Flips the image horizontally or vertically.\n",
    "- **RandomRotation**: Rotates the image by a random angle within a specified range.\n",
    "- **RandomZoom**: Zooms in or out of the image.\n",
    "- **RandomContrast**: Adjusts the contrast of the image randomly.\n",
    "\n",
    "#### Visualizing Augmented Images\n",
    "To better understand each augmentation, we will visualize the effects of individual transformations. For each augmentation layer, we’ll show:\n",
    "1. The **original image**.\n",
    "2. The **augmented image** after applying the transformation.\n"
   ],
   "id": "91421debc0a1a1dc"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "id": "c65665a8752bf57f",
    "outputId": "0b6413f0-364b-403a-bafb-efcd3604d770"
   },
   "cell_type": "code",
   "source": [
    "# Define individual augmentation layers\n",
    "augmentation_layers = {\n",
    "    \"RandomCrop\": RandomCrop(200, 200),  # Crop image to 200x200\n",
    "    \"RandomFlip\": RandomFlip(\"horizontal_and_vertical\"),  # Flip horizontally and vertically\n",
    "    \"RandomRotation\": RandomRotation(0.2),  # Rotate by up to 20%\n",
    "    \"RandomZoom\": RandomZoom(0.2, 0.2),  # Zoom in or out\n",
    "    \"RandomContrast\": RandomContrast(0.2),  # Adjust contrast\n",
    "}\n",
    "\n",
    "# Select one image to demonstrate augmentations\n",
    "for images, labels in normalized_train_dataset.take(1):\n",
    "    original_image = images[0].numpy().astype(\"float32\")\n",
    "    break\n",
    "\n",
    "# Plot original image and augmented versions\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i, (name, layer) in enumerate(augmentation_layers.items()):\n",
    "    augmented_image = layer(tf.expand_dims(original_image, 0))  # Apply augmentation\n",
    "    augmented_image = augmented_image[0].numpy()  # Remove batch dimension\n",
    "    augmented_image = tf.clip_by_value(augmented_image, 0.0, 1.0).numpy()  # Clip values to [0, 1]\n",
    "\n",
    "    # Display original image\n",
    "    ax = plt.subplot(len(augmentation_layers), 2, i * 2 + 1)\n",
    "    plt.imshow(original_image)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Display augmented image\n",
    "    ax = plt.subplot(len(augmentation_layers), 2, i * 2 + 2)\n",
    "    plt.imshow(augmented_image)\n",
    "    plt.title(name)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c65665a8752bf57f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model Selection: MobileNetV2 Pre-trained on ImageNet\n",
    "\n",
    "When training a model for a specific task, it’s often easier to start with an **existing model** that has already been trained on a large, general-purpose dataset. These models are called **pre-trained models**, and they save a lot of time and computational resources because they have already learned to recognize patterns and features from a wide range of images.\n",
    "\n",
    "In this lab, we’re using **MobileNetV2**, a pre-trained model that was trained on a dataset called **ImageNet**. ImageNet is a collection of over 1 million images grouped into 1,000 categories, such as dogs, cats, elephants, and more. The MobileNetV2 model has learned to identify these categories very well.\n",
    "\n",
    "Before we adapt MobileNetV2 for our defect detection task, let’s see how it works on its original training data by:\n",
    "1. Loading the pre-trained MobileNetV2 model.\n",
    "2. Testing it on a few sample images from ImageNet-like categories.\n",
    "3. Viewing its predictions to understand what it has already learned.\n",
    "\n",
    "---\n",
    "\n",
    "### What is MobileNetV2 Doing?\n",
    "\n",
    "MobileNetV2 is like a complex decision-making process:\n",
    "1. **Lower Layers**: Detect simple features, like edges or textures.\n",
    "2. **Middle Layers**: Combine these simple features into more complex ones, like shapes or objects.\n",
    "3. **Top Layers**: Decide on the most likely category (e.g., “dog,” “cat”) using what it has learned during training.\n",
    "\n",
    "Right now, the MobileNetV2 model is trained to identify objects in ImageNet categories, but we’ll see how it performs by testing it on a few example images.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Load the Pre-trained MobileNetV2 Model\n",
    "\n",
    "The `MobileNetV2` model is available in `tf.keras.applications`, and we can load it with just one line of code:"
   ],
   "id": "e952fec14318aad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load MobileNetV2 with the original classification head\n",
    "imagenet_model = MobileNetV2(weights='imagenet', include_top=True)"
   ],
   "id": "429eb92ae43a7446",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2: Load and Preprocess Sample Images\n",
    "\n",
    "To use MobileNetV2, the input images must be prepared in a specific way:\n",
    "\n",
    "1. Resize the images to 224 × 224 pixels (the size MobileNetV2 expects).\n",
    "2. Normalize the pixel values to match how the model was trained.\n",
    "\n",
    "We’ll use some sample images of objects like a dog, a cat, and an elephant to test the model:"
   ],
   "id": "279f99deb7f7ec86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "urls = {\n",
    "    \"cat\":\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b6/Felis_catus-cat_on_snow.jpg/1920px-Felis_catus-cat_on_snow.jpg\",\n",
    "    \"dog\":\"https://upload.wikimedia.org/wikipedia/commons/9/99/Brooks_Chase_Ranger_of_Jolly_Dogs_Jack_Russell.jpg\",\n",
    "    \"elephant\":\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/African_Bush_Elephant.jpg/1024px-African_Bush_Elephant.jpg\"\n",
    "}\n",
    "\n",
    "# Load and preprocess the sample images\n",
    "sample_images = {}\n",
    "for name, url in urls.items():\n",
    "    img_path = tf.keras.utils.get_file(f\"{name}.jpg\", url)\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    sample_images[name] = img_array\n",
    "\n",
    "# Display the sample images\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, (name, img_array) in enumerate(sample_images.items()):\n",
    "    ax = plt.subplot(1, 3, i + 1)\n",
    "    plt.imshow((img_array+1)/2) # Rescale pixel values to [0, 1] - in MobileNetV2 the range is [-1, 1]\n",
    "    plt.title(name)\n",
    "    plt.axis(\"off\")"
   ],
   "id": "851e298fbf50ff36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3: Use MobileNetV2 to Make Predictions\n",
    "\n",
    "Now let’s use the model to predict what it sees in the images. The predictions will be probabilities for each of the 1,000 categories, and we’ll decode these to show the most likely categories:"
   ],
   "id": "c8baaa35104282ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make predictions on the sample images\n",
    "predictions = {}\n",
    "for name, img_array in sample_images.items():\n",
    "    predictions[name] = decode_predictions(imagenet_model.predict(np.expand_dims(img_array, axis=0)), top=3)\n",
    "\n",
    "# Display the predictions\n",
    "for name, preds in predictions.items():\n",
    "    print(f\"Predictions for {name}:\")\n",
    "    for pred in preds[0]:\n",
    "        print(f\"  {pred[1]}: {pred[2]*100:.0f}%\")\n",
    "    print()"
   ],
   "id": "3f93ba1a83f38403",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "be1d4234eaac6d64"
   },
   "cell_type": "markdown",
   "source": [
    "### Why Is This Helpful?\n",
    "\n",
    "You might be wondering: if MobileNetV2 is trained to recognize objects like dogs, cats, and elephants, how does this help us distinguish between defective and healthy auto parts? The key lies in understanding **feature extraction** and how pre-trained models like MobileNetV2 learn to recognize patterns.\n",
    "\n",
    "#### What MobileNetV2 Has Learned\n",
    "1. **Low-Level Features**: The early layers of MobileNetV2 detect basic patterns like edges, corners, and textures. These features are universal—they’re useful for understanding any image, whether it’s a dog or an auto part.\n",
    "2. **High-Level Features**: The deeper layers combine these patterns into more complex shapes, like wheels or fur. These features are specific to the ImageNet dataset and won’t directly help with our defect detection task.\n",
    "\n",
    "By reusing the **low-level feature extraction** layers and replacing the high-level layers with new ones tailored to our dataset, we save time and computational effort. Instead of training from scratch, we only need to teach the model how to use these general features to distinguish defective parts from healthy ones.\n",
    "\n",
    "---\n",
    "\n",
    "### Visualizing the Receptive Fields of Early Layers\n",
    "\n",
    "To see why these low-level features are useful, let’s visualize what the **early layers** of MobileNetV2 focus on when processing an image. These layers form the building blocks of the model’s understanding of visual data.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: Load the Base Model\n",
    "We’ll load the MobileNetV2 model but stop at an early layer to analyze the features it extracts:"
   ],
   "id": "be1d4234eaac6d64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load MobileNetV2\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False)\n",
    "layer_name = 'block_1_expand'  # Choose an early layer\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=base_model.get_layer(layer_name).output)"
   ],
   "id": "f64e77ca0ee0ca8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2: Visualize Receptive Fields\n",
    "\n",
    "Let’s pass an image through the feature extractor and visualize the activation maps. These maps show which parts of the image the model focuses on at different positions."
   ],
   "id": "4810f1a03f04985b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "img_array = sample_images[\"cat\"]  # Choose an image to visualize\n",
    "\n",
    "# Get the activation maps\n",
    "activations = feature_extractor.predict(np.expand_dims(img_array, axis=0))\n",
    "\n",
    "# Visualize the activation maps\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(5):\n",
    "    ax = plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(activations[0, :, :, i], cmap='viridis')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Activation Map {i}\")"
   ],
   "id": "47422112654acf7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### What Do We See?\n",
    "\n",
    "The activation maps highlight edges, corners, and textures in the image. These low-level patterns are essential for understanding any visual data, including auto parts. Even though the model was trained on ImageNet, these universal features apply to other tasks, like identifying defects in auto parts.\n",
    "\n",
    "### Connecting This to Auto Parts\n",
    "\n",
    "When distinguishing defective auto parts, the model doesn’t need to know what the part is—it just needs to identify patterns that differentiate a healthy part from a defective one. The low-level features learned by MobileNetV2:\n",
    "\n",
    "1. Help detect scratches, cracks, or texture anomalies.\n",
    "2. Provide a foundation for recognizing visual inconsistencies.\n",
    "\n",
    "By keeping these early layers and fine-tuning the top layers, we can adapt MobileNetV2 to focus on defect detection without starting from scratch.\n",
    "\n",
    "This reuse of pre-trained features is what makes transfer learning so powerful, saving time and making the model both efficient and effective."
   ],
   "id": "4cfd18578e8bc09c"
  },
  {
   "metadata": {
    "id": "6d9ec971828a86c0"
   },
   "cell_type": "markdown",
   "source": [
    "# Model Selection\n",
    "\n",
    "With this intro in mind, we are now ready to set up the model for defect detection. We will use the MobileNetV2 architecture as the base model and fine-tune it for our specific task. The model will be trained to classify images into two categories: **defective** and **healthy** parts.\n",
    "\n",
    "## Step 1: Load the Pre-trained Model\n",
    "\n",
    "We begin by loading MobileNetV2 with pre-trained weights. Since our task is different from the original ImageNet classification, we’ll:\n",
    "1. Remove the top layers of MobileNetV2 (responsible for ImageNet-specific classifications).\n",
    "2. Keep the base layers, which extract general visual features."
   ],
   "id": "6d9ec971828a86c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load MobileNetV2 without the top classification layers\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,  # Exclude ImageNet-specific top layers\n",
    "    weights='imagenet'  # Use pre-trained weights\n",
    ")\n",
    "\n",
    "# Freeze layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# We are also going to pre-process our images to be between [-1, 1] which is what MobileNetV2 expects\n",
    "def preprocess(image, label):\n",
    "    image = preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess)\n",
    "test_dataset = test_dataset.map(preprocess)"
   ],
   "id": "24069c39404ca5aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Why freeze the base layers? We want to retain the general features learned from ImageNet without altering them. Instead, we’ll train new layers on top to focus on our specific task.\n",
    "\n",
    "### Step 2: Add Task-Specific Layers\n",
    "\n",
    "To adapt MobileNetV2 to defect detection, we’ll add new layers:\n",
    "\n",
    "- **Global Average Pooling**: Reduces the feature maps from the base model into a single vector, summarizing important information.\n",
    "- **Fully Connected Layer**: Adds additional learnable parameters for our task.\n",
    "- **Output Layer**: Produces a single probability (0 = healthy, 1 = defective) using a **sigmoid** activation."
   ],
   "id": "87a3a506804b0696"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build the complete model\n",
    "model = Sequential([\n",
    "    Input(shape=(224, 224, 3)),       # Input shape matches the pre-trained model\n",
    "    base_model,                       # Use the frozen MobileNetV2 base\n",
    "    GlobalAveragePooling2D(),         # Reduce spatial dimensions to a single vector\n",
    "    Dense(128, activation='relu'),    # Learnable dense layer\n",
    "    Dense(64, activation='relu'),     # Additional dense layer\n",
    "    Dense(1, activation='sigmoid')    # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Display the model structure\n",
    "model.summary()"
   ],
   "id": "df1782514d759a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notice that unlike yesterday, we now have a distinction between **trainable** and **non-trainable** parameters. This means that we have instructed Keras not to update the weights of the MobileNetV2 base model during training. Instead, only the new layers we added will be trained to classify defective and healthy parts.\n",
    "\n",
    "### Step 4: Compile the Model\n",
    "\n",
    "Next, we compile the model to define:\n",
    "\n",
    "- Loss Function: binary_crossentropy for binary classification. Binary Cross-Entropy calculates the error based on *how confident* the model is in its predictions. In principle, the model should only ever be *very confident* about a good prediction, or failing that, *not very confident* at all. It's calculated as follows:\n",
    "\n",
    "$$\n",
    "\\text{Binary Cross-Entropy} = -\\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)\n",
    "$$\n",
    "\n",
    "- Optimizer: For now we'll stick with Stochastic Gradient Descent (SGD), but you can experiment with other optimizers like Adam or RMSprop at the end of the lab.\n",
    "- Metrics: Accuracy, to monitor performance."
   ],
   "id": "1216a46589572463"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ],
   "id": "8595adb0dcf46a4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 5: Train the Model\n",
    "\n",
    "Finally, we train the model on the dataset for 15 epochs and validate its performance. We will also optimize data loading using Keras' `prefetch` method, which loads data in the background while the model is training."
   ],
   "id": "bd89d8f0a1f6685e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optimize data loading for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=15,\n",
    "    validation_data=val_dataset\n",
    ")"
   ],
   "id": "9308094e8d7e6dcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the validation accuracy\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "29c145dcda51bed5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluating Model Performance: Detailed Breakdown\n",
    "\n",
    "Now that the model is trained, let’s take a closer look at its performance on the **testing set**. While accuracy gives us a general idea of how well the model is doing, it’s often helpful to use other metrics to better understand its behavior. We’ll compute:\n",
    "1. **Confusion Matrix**: A summary of true positives, true negatives, false positives, and false negatives.\n",
    "2. **Precision and Recall**: To understand how well the model identifies defective parts.\n",
    "3. **F1-Score**: A harmonic mean of precision and recall, giving a single metric that balances the two.\n",
    "\n",
    "Let’s also visualize the confusion matrix to make the results more interpretable."
   ],
   "id": "dee93716fe129ec5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Predict on the testing set\n",
    "test_images, test_labels = [], []\n",
    "for images, labels in test_dataset:\n",
    "    test_images.append(images)\n",
    "    test_labels.append(labels)\n",
    "test_images = np.concatenate(test_images, axis=0)\n",
    "test_labels = np.concatenate(test_labels, axis=0)\n",
    "\n",
    "predictions = model.predict(test_images)\n",
    "predicted_labels = (predictions > 0.5).astype(int).flatten()  # Convert probabilities to binary predictions\n",
    "\n",
    "# Step 2: Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "# Step 3: Display the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Healthy', 'Defective'], yticklabels=['Healthy', 'Defective'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Generate the classification report\n",
    "report = classification_report(test_labels, predicted_labels, target_names=['Healthy', 'Defective'])\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ],
   "id": "b6426c780fcbf240",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Analyzing Misclassified and Low-Confidence Samples\n",
    "\n",
    "To better understand the model’s performance, let’s inspect:\n",
    "1. **Misclassified Samples**: Images that the model got wrong. This helps us identify patterns in its mistakes.\n",
    "2. **Low-Confidence Correct Predictions**: Images that were classified correctly but with a low confidence score. These can reveal edge cases where the model is uncertain.\n",
    "\n",
    "We will visualize these samples alongside their predicted labels and confidence scores to understand where the model struggles and why."
   ],
   "id": "99c92bf697a7d057"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Predict on the testing set\n",
    "predictions = model.predict(test_images).flatten()\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "confidence_scores = np.where(predictions < 0.5, 1 - predictions, predictions)\n",
    "\n",
    "# Step 2: Identify misclassified samples\n",
    "misclassified_indices = np.where(predicted_labels != test_labels)[0]\n",
    "\n",
    "# Step 3: Identify low-confidence correct samples\n",
    "low_confidence_correct_indices = np.where(\n",
    "    (predicted_labels == test_labels) & (confidence_scores < 0.75)\n",
    ")[0]\n",
    "\n",
    "# Step 4: Define a function to visualize samples\n",
    "def visualize_samples(indices, title, num_samples=9):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i, idx in enumerate(indices[:num_samples]):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow((test_images[idx] + 1)/2)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            f\"True: {'Defective' if test_labels[idx] else 'Healthy'}\\n\"\n",
    "            f\"Pred: {'Defective' if predicted_labels[idx] else 'Healthy'}\\n\"\n",
    "            f\"Confidence: {confidence_scores[idx]:.2f}\"\n",
    "        )\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Step 5: Visualize misclassified samples\n",
    "print(f\"Total misclassified samples: {len(misclassified_indices)}\")\n",
    "visualize_samples(misclassified_indices, \"Misclassified Samples\")\n",
    "\n",
    "# Step 6: Visualize low-confidence correct samples\n",
    "print(f\"Total low-confidence correct samples: {len(low_confidence_correct_indices)}\")\n",
    "visualize_samples(low_confidence_correct_indices, \"Low-Confidence Correct Samples\")"
   ],
   "id": "f1279deb954d2c25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Comparing Activation Maps: Pre-trained vs. Fine-tuned Model\n",
    "\n",
    "Now that our model has been fine-tuned, let’s revisit the **activation maps** from the `block_1_expand` layer. This time, we’ll compare how the pre-trained MobileNetV2 and the fine-tuned model process:\n",
    "1. A **cat image** (used earlier to observe pre-trained activations).\n",
    "2. An image from our **defect detection dataset**.\n",
    "\n",
    "This comparison helps us understand how the model’s feature extraction has adapted to the new task."
   ],
   "id": "c74acd91d1412b87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 1: Define the fine-tuned feature extractor using the base model\n",
    "fine_tuned_feature_extractor = Model(\n",
    "    inputs=base_model.input,  # Use the base model's input\n",
    "    outputs=base_model.get_layer('block_1_expand').output  # Extract the output of the desired layer\n",
    ")\n",
    "\n",
    "# Step 2: Preprocess the cat image\n",
    "cat_array = sample_images[\"cat\"]\n",
    "cat_array = np.expand_dims(cat_array, axis=0)\n",
    "\n",
    "# Step 3: Select an image from the defect detection dataset\n",
    "for defect_images, _ in test_dataset.take(1):  # Grab a batch\n",
    "    defect_image = defect_images[0].numpy()  # Take the first image\n",
    "    break\n",
    "defect_array = np.expand_dims(defect_image, axis=0)\n",
    "\n",
    "# Step 4: Generate activation maps\n",
    "cat_activations = fine_tuned_feature_extractor.predict(cat_array)\n",
    "defect_activations = fine_tuned_feature_extractor.predict(defect_array)\n",
    "\n",
    "# Step 5: Visualize activation maps\n",
    "def visualize_activation_maps(activations, title, num_maps=8):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Display activation maps\n",
    "    for i in range(num_maps):\n",
    "        plt.subplot(2, num_maps, i + 1)\n",
    "        plt.imshow(activations[0, :, :, i], cmap='viridis')\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Map {i + 1}\")\n",
    "\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize activations for the cat image\n",
    "visualize_activation_maps(cat_activations, \"Fine-Tuned Activations: Cat Image\")\n",
    "\n",
    "# Visualize activations for the defect detection image\n",
    "visualize_activation_maps(defect_activations, \"Fine-Tuned Activations: Defect Detection Image\")"
   ],
   "id": "928969ccfa8da2c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Real-World Application: Defect Detection Alert System\n",
    "\n",
    "In a real-world scenario, a trained model can be deployed as part of an automated quality control pipeline. For example, in a manufacturing environment, the model can evaluate images of parts and provide alerts if defects are detected with high confidence.\n",
    "\n",
    "Here, we’ll implement a simple **detector function** that:\n",
    "1. Accepts an input image and a confidence threshold.\n",
    "2. Predicts whether the image is defective.\n",
    "3. Returns an \"alert\" if the defect probability exceeds the threshold."
   ],
   "id": "39c31e456fd0bc0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def detect_defect_from_dataset(image_index, dataset, model, threshold=0.25):\n",
    "    \"\"\"\n",
    "    Detects defects in an image from a dataset using the trained model.\n",
    "\n",
    "    Parameters:\n",
    "    - image_index (int): Index of the image in the dataset to evaluate.\n",
    "    - dataset (tf.data.Dataset): The dataset containing the images to evaluate.\n",
    "    - model (keras.Model): The trained defect detection model.\n",
    "    - threshold (float): Confidence threshold for triggering an alert.\n",
    "\n",
    "    Returns:\n",
    "    - str: Result message with defect probability and alert status.\n",
    "    \"\"\"\n",
    "    # Extract the specified image and its label\n",
    "    dataset_images, dataset_labels = [], []\n",
    "    for images, labels in dataset:\n",
    "        dataset_images.append(images)\n",
    "        dataset_labels.append(labels)\n",
    "    dataset_images = np.concatenate(dataset_images, axis=0)\n",
    "    dataset_labels = np.concatenate(dataset_labels, axis=0)\n",
    "\n",
    "    img = dataset_images[image_index]\n",
    "    true_label = dataset_labels[image_index]\n",
    "\n",
    "    # Preprocess the image (add batch dimension)\n",
    "    img_array = np.expand_dims(img, axis=0)\n",
    "\n",
    "    # Step 1: Make a prediction\n",
    "    prediction = model.predict(img_array)[0][0]  # Get defect probability (scalar)\n",
    "\n",
    "    # Step 2: Determine result based on the threshold\n",
    "    if prediction < threshold:\n",
    "        result = f\"ALERT: Defect detected with probability {prediction:.2f} (Threshold: {threshold:.2f})\"\n",
    "    else:\n",
    "        result = f\"OK: No defect detected (Probability: {prediction:.2f}, Threshold: {threshold:.2f})\"\n",
    "\n",
    "    return result, img, true_label, prediction\n",
    "\n",
    "# Example usage\n",
    "image_index = 2  # Index of the image to evaluate\n",
    "threshold = 0.25  # Define a confidence threshold\n",
    "result, img, true_label, prediction = detect_defect_from_dataset(image_index, test_dataset, model, threshold)\n",
    "\n",
    "# Display the result\n",
    "print(result)\n",
    "\n",
    "# Visualize the image\n",
    "plt.imshow((img + 1)/2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"True Label: {'Defective' if true_label else 'Healthy'}\\nPrediction: {prediction:.2f}\")\n",
    "plt.show()"
   ],
   "id": "12057d0c3b130115",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "In this lab, we took a deep dive into applying **transfer learning** and **computer vision** techniques to detect defects in automotive parts. By building on the foundational skills introduced earlier in the workshop, we explored advanced workflows for image classification and model adaptation.\n",
    "\n",
    "Here’s a summary of what we covered:\n",
    "\n",
    "1. **Dataset Exploration and Preprocessing**:\n",
    "   - Loaded and visualized the **Fender Apron** dataset to understand the characteristics of defective and healthy parts.\n",
    "   - Performed preprocessing steps including resizing, normalization, and data augmentation to prepare the dataset for training.\n",
    "\n",
    "2. **Understanding Pre-trained Models**:\n",
    "   - Explored how MobileNetV2, pre-trained on ImageNet, extracts features and how these features can be adapted to a new task.\n",
    "   - Visualized activation maps from early layers to see what the model focuses on before and after fine-tuning.\n",
    "\n",
    "3. **Model Training and Fine-tuning**:\n",
    "   - Used the MobileNetV2 architecture as the base model and fine-tuned it for binary classification (defective vs. healthy).\n",
    "   - Optimized the training process with data augmentation and prefetching for performance improvements.\n",
    "\n",
    "4. **Evaluation and Insights**:\n",
    "   - Analyzed the model's performance using metrics like precision, recall, F1-score, and confusion matrices.\n",
    "   - Inspected misclassified and low-confidence samples to identify patterns and potential edge cases.\n",
    "\n",
    "5. **Real-world Applications**:\n",
    "   - Implemented a simple defect detection alert system that could be used in automated quality control pipelines.\n",
    "   - Demonstrated how a trained model can make predictions on test images and trigger alerts based on confidence thresholds.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Transfer learning** leverages pre-trained models to save time and computational resources, making it an effective approach for specialized tasks like defect detection.\n",
    "- Visualizing activation maps and misclassified samples provides deeper insights into model behavior and areas for improvement.\n",
    "- The combination of data augmentation, efficient preprocessing, and fine-tuning enables the creation of robust and generalizable models.\n",
    "\n",
    "As we move forward in this workshop, you are encouraged to:\n",
    "- Experiment with different architectures and data augmentation strategies.\n",
    "- Investigate ways to improve the model’s performance on edge cases or ambiguous samples.\n",
    "- Consider deployment strategies for integrating defect detection models into real-world systems.\n",
    "\n",
    "---\n",
    "\n",
    "### Bonus Exercise: Trying a Different Pre-trained Model\n",
    "\n",
    "To deepen your understanding of transfer learning and model fine-tuning, try swapping out MobileNetV2 with a different pre-trained model. This exercise will help you explore the flexibility of pre-trained models and understand the trade-offs between different architectures.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: Choose a New Pre-trained Model\n",
    "\n",
    "Keras provides several pre-trained models in the `tf.keras.applications` module. Some popular choices include:\n",
    "- **ResNet50**: A deeper architecture designed to learn complex patterns, known for its residual connections.\n",
    "- **InceptionV3**: A model that uses a creative \"Inception\" block to capture features at multiple scales.\n",
    "- **EfficientNetB0**: A highly optimized model that balances accuracy and computational efficiency.\n",
    "\n",
    "Choose one of these models based on your goals:\n",
    "- If you want higher accuracy and don’t mind a longer training time, try **ResNet50** or **InceptionV3**.\n",
    "- If you need efficiency and speed, go with **EfficientNetB0**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Replace MobileNetV2 with Your Chosen Model\n",
    "\n",
    "1. Import your chosen model from `tf.keras.applications`.\n",
    "2. Replace MobileNetV2 in the model setup:\n",
    "   - Ensure you set `include_top=False` to exclude the original classification head.\n",
    "   - Specify the input shape `(224, 224, 3)` (or modify it to match your dataset).\n",
    "3. Freeze the base layers of the new model so you can fine-tune only the top layers.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Adjust Preprocessing\n",
    "\n",
    "Each pre-trained model has specific preprocessing requirements:\n",
    "- Use the corresponding `preprocess_input` function from `tf.keras.applications.<model>` to normalize your dataset correctly.\n",
    "- For example:\n",
    "  - ResNet50 and InceptionV3 require inputs scaled to [-1, 1].\n",
    "  - EfficientNetB0 requires inputs scaled to [0, 1].\n",
    "\n",
    "Update your preprocessing pipeline to match the model’s expectations.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Add New Task-Specific Layers\n",
    "\n",
    "Attach new layers to adapt the model to your defect detection task:\n",
    "1. Use a **GlobalAveragePooling2D** layer to reduce spatial dimensions.\n",
    "2. Add one or more **Dense** layers for additional learning.\n",
    "3. Use a **Dense(1, activation='sigmoid')** layer as the output for binary classification.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 5: Compile and Train\n",
    "\n",
    "1. Compile the model using the same loss function (`binary_crossentropy`) and optimizer (`Adam`).\n",
    "2. Train the model on the same dataset for 15 epochs.\n",
    "3. Monitor the training and validation accuracy to compare performance with MobileNetV2.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 6: Evaluate and Compare Results\n",
    "\n",
    "After training, evaluate the model on the testing set using:\n",
    "1. Accuracy, precision, recall, and F1-score.\n",
    "2. Confusion matrices and misclassified samples.\n",
    "\n",
    "Compare the results with those from MobileNetV2:\n",
    "- Did the new model perform better or worse?\n",
    "- Was the training time noticeably different?\n",
    "- Were there any patterns in the errors or low-confidence predictions?"
   ],
   "id": "613974a08f8401a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your Code Here",
   "id": "c47c6ef9bf5a1eb5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
