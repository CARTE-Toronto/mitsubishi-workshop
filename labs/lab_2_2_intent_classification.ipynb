{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AI Workshop - Lab 2-2: Intent Classification\n",
    "\n",
    "In this lab, we’ll build a system to classify customer text messages into different categories (called **intents**) using a powerful type of AI model called a transformer. Transformers are a key technology behind tools like ChatGPT and other modern language systems, but don’t worry if you’re new to them—we’ll break it down step by step.\n",
    "\n",
    "### Data Overview\n",
    "\n",
    "We’re working with a dataset of customer text messages that has already been labeled with their intent (e.g., \"Order Status\", \"Product Inquiry\", \"Account Help\"). The goal is to teach the model to recognize these patterns so it can classify new messages correctly.\n",
    "\n",
    "- **Dataset**:\n",
    "  - Provided as two files: one for training and one for testing.\n",
    "  - Training data is used to teach the model, and testing data is used to see how well it learned.\n",
    "- **Number of Categories**: 27 different intents.\n",
    "\n",
    "### What We’ll Do in This Lab\n",
    "1. **Load the Data**:\n",
    "   - Open and inspect the dataset to understand its structure.\n",
    "   - Check how many examples we have for each intent.\n",
    "2. **Prepare the Data**:\n",
    "   - Use a tool called a **tokenizer** to break down text messages into a format the model can understand.\n",
    "   - Convert the intent labels into numbers so the model can learn from them.\n",
    "3. **Use a Pre-Trained Model**:\n",
    "   - Start with an existing model called `T5-small` that already knows a lot about language.\n",
    "   - Customize (or fine-tune) it to focus on the intents in our dataset.\n",
    "4. **Train the Model**:\n",
    "   - Use the prepared data to train the model step by step.\n",
    "   - Measure how well it’s doing along the way.\n",
    "5. **Evaluate the Model**:\n",
    "   - Test the model on new data it hasn’t seen before.\n",
    "   - Check how accurate it is and where it might make mistakes.\n",
    "\n",
    "### What You’ll Learn\n",
    "- **Transformers**: Get an introduction to these models and why they’re so powerful for language tasks.\n",
    "- **Fine-Tuning**: Learn how to take a pre-trained model and adapt it to solve a specific problem.\n",
    "- **Model Evaluation**: Understand how to measure a model’s performance and interpret its predictions.\n",
    "\n",
    "### HuggingFace Libraries\n",
    "\n",
    "So far we have been working with Keras, a popular library for building neural networks. In this lab, we’ll use the HuggingFace libraries, which are designed specifically for working with transformers.\n",
    "\n",
    "The main HuggingFace library is called `transformers`, and it provides tools for working with pre-trained models, tokenizers, and training pipelines. We’ll also use `datasets` to load and process our data. `accelerate` and `evaluate` are additional libraries that help speed up training and evaluate models, respectively. Install them below:"
   ],
   "id": "89800ddaafaa2296"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -Uq datasets transformers accelerate evaluate",
   "id": "3d3c731f7713c24a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For this lab, it's essential that we have a GPU available to speed up training. On Google Colab, you can enable a GPU by going to **Runtime** > **Change runtime type** > **Hardware accelerator** > **GPU**.\n",
    "\n",
    "The following line of code will check if a GPU is available:"
   ],
   "id": "1bcffd8486a0820c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print('GPU is available!')\n",
    "else:\n",
    "    print('GPU is not available. Enable a GPU runtime in Colab under \"Runtime\" > \"Change runtime type\".')"
   ],
   "id": "967b287ee6c7830c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "Now that we’ve set up our environment and imported the necessary packages, let’s begin by loading our dataset.\n",
    "\n",
    "In this lab, we’ll work with a dataset of **customer text messages** that have been labeled with their **intent**. Each sample in the dataset includes a text message and a corresponding label indicating the intent behind the message (e.g., inquiry, complaint, order request). This dataset will allow us to build and evaluate models for intent classification.\n",
    "\n",
    "#### Steps:\n",
    "1. **Load the Dataset**:\n",
    "   - Use the `load_dataset` function from the `datasets` library to download and load the dataset.\n",
    "   - The dataset we’re using is hosted at `\"alexwaolson/customer-intents\"`.\n",
    "2. **Inspect the Dataset**:\n",
    "   - After loading, examine the training split (`intents['train']`) to understand its structure and the data it contains."
   ],
   "id": "b3a8dbd2fdcd2d6b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the customer intents dataset\n",
    "intents = load_dataset(\"alexwaolson/customer-intents\")\n",
    "\n",
    "# Display the training split\n",
    "intents['train']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The dataset consists of two key columns:\n",
    "- **`message`**: Contains the text of the customer message.\n",
    "- **`label`**: Contains the intent category for each message.\n",
    "\n",
    "There are **27 possible intent categories** in this dataset. To understand the distribution of these categories, we can count the number of examples for each intent. This helps us determine whether the dataset is balanced (i.e., whether all categories have similar representation) or imbalanced (some categories have significantly more or fewer samples than others).\n",
    "\n",
    "Run the code below to calculate the distribution of intent labels:"
   ],
   "id": "9b2a9381462eaa7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the occurrences of each intent label in the training data\n",
    "label_counts = Counter(intents['train']['label'])\n",
    "print(f'Number of unique intents: {len(label_counts)}')\n",
    "\n",
    "# Plot the distribution of intent labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(label_counts.keys(), label_counts.values())\n",
    "plt.xlabel('Intent Label')\n",
    "plt.ylabel('Number of Examples')\n",
    "plt.title('Distribution of Intent Labels')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "id": "4b11c9f3d9801d3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocessing the Dataset\n",
    "\n",
    "Now that our dataset is loaded, we need to prepare it for training. This involves several steps, including tokenization, padding, and setting up a data collator. These steps ensure that the raw text data is transformed into a format that our model can understand and process efficiently.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Most language models, including T5, don't work directly with raw text but with **tokenized inputs**. Tokenization involves breaking down text into smaller units, called **tokens**, which are then converted into numerical IDs that the model can process.\n",
    "\n",
    "#### How Tokenization Works:\n",
    "1. **Splitting Text**:\n",
    "   - Text is split into words, subwords, or characters.\n",
    "   - For example, the word \"running\" might be split into \"run\" and \"##ning\" (where `##` indicates a suffix).\n",
    "2. **Mapping to IDs**:\n",
    "   - Each token is mapped to a unique integer ID using a vocabulary associated with the tokenizer.\n",
    "3. **Flexibility**:\n",
    "   - This approach allows the model to understand the meaning of individual components (like \"run\" and \"-ning\") and combine them to interpret words.\n",
    "\n",
    "#### Why Tokenization is Important:\n",
    "- It converts raw text into numerical data that can be used for deep learning.\n",
    "- It helps the model generalize to unseen words by breaking them into familiar components.\n",
    "\n",
    "#### T5 Model and Tokenizer:\n",
    "We’ll use the `AutoTokenizer` class from the `transformers` library to load a tokenizer that matches the **T5-small** model. T5 (Text-to-Text Transfer Transformer) is a versatile language model designed to handle a wide range of NLP tasks by treating all tasks as text-to-text transformations.\n",
    "\n",
    "Let's load the tokenizer and inspect its properties:"
   ],
   "id": "88b3c3de6ae442f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load our tokenizer\n",
    "model_name = 't5-small'\n",
    "# The AutoTokenizer class will automatically select the correct tokenizer class for the model!\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "id": "c1d73e9530267d08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Inspecting the Tokenizer\n",
    "\n",
    "Before we start using the tokenizer to preprocess our dataset, let’s take a closer look at its capabilities. Understanding the tokenizer’s vocabulary and special tokens will help us use it effectively.\n",
    "\n",
    "#### Vocabulary Size\n",
    "The tokenizer’s **vocabulary size** represents the total number of unique tokens it can recognize and map to IDs. This includes:\n",
    "- Words\n",
    "- Subwords\n",
    "- Special tokens\n",
    "\n",
    "A larger vocabulary allows the model to understand a wider range of text but may also increase the computational requirements.\n",
    "\n",
    "#### Special Tokens\n",
    "Special tokens are reserved tokens used for specific purposes in a model:\n",
    "- **Start and End Tokens**: Indicate the beginning and end of sequences.\n",
    "- **Padding Token**: Ensures all sequences in a batch have the same length by filling shorter sequences.\n",
    "- **Other Tokens**: For tasks like separating sentences or marking different segments in the input."
   ],
   "id": "23d56975de7a7578"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inspect the tokenizer's vocabulary and special tokens\n",
    "print(f'Vocab size:     {tokenizer.vocab_size} tokens')  # Total number of tokens in the vocabulary\n",
    "print(f'End token:      {tokenizer.eos_token}')          # End of sequence token\n",
    "print(f'Padding token:  {tokenizer.pad_token}')          # Padding token\n",
    "print(f'Unknown token:  {tokenizer.unk_token}')          # Unknown token, for out-of-vocabulary words"
   ],
   "id": "96ef738b67675e92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The tokenizer not only converts text into token IDs but can also convert token IDs back into text. This process involves:\n",
    "- **Encoding**: Breaking a string into tokens and mapping them to unique IDs.\n",
    "- **Decoding**: Converting token IDs back into a human-readable string.\n",
    "\n",
    "To better understand how tokenization works, we’ll use a custom function, `show_tokenization`, which:\n",
    "1. Displays the original text.\n",
    "2. Encodes the text into token IDs.\n",
    "3. Decodes each token ID back into its corresponding token.\n",
    "\n",
    "#### **Your Turn**:\n",
    "- Write a sentence in the `show_tokenization` function to see how it is tokenized by the model.\n",
    "- Try using:\n",
    "  - A sentence with common words.\n",
    "  - A sentence containing made-up words.\n",
    "  - Words that might be split into multiple tokens (e.g., \"unbelievably\")."
   ],
   "id": "d31cbc545d9c61ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def show_tokenization(tokenizer, text):\n",
    "    print(f'Original text: {text}')\n",
    "    tokens = tokenizer(text, truncation=True)['input_ids']\n",
    "    for token in tokens:\n",
    "        print(f'{tokenizer.decode([token]):10} -> {token}')\n",
    "\n",
    "# Write a sentence to see how it gets tokenized:\n",
    "show_tokenization(tokenizer, 'your sentence here')"
   ],
   "id": "33041b43d6be5ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenizer Basics: Padding and Attention Masks\n",
    "\n",
    "Now that you’ve seen how tokenization works, let’s cover a few additional concepts: **attention masks** and **padding**. These concepts are crucial for processing variable-length sequences when training models.\n",
    "\n",
    "#### Attention Mask\n",
    "- The **attention mask** is a vector that tells the model which tokens are actual input and which are padding.\n",
    "- For example:\n",
    "  - A token with a mask value of `1` indicates that it is part of the original input.\n",
    "  - A token with a mask value of `0` indicates that it is a padding token and should be ignored during processing.\n",
    "\n",
    "You may see this term pop up later when preparing data for the model.\n",
    "\n",
    "#### Padding\n",
    "When training models in batches, all sequences in a batch must have the same length to allow parallel processing. Since sequences vary in length, we:\n",
    "1. Add **padding tokens** to the shorter sequences to make them the same length as the longest sequence in the batch.\n",
    "2. Use the attention mask to ensure the model ignores these padding tokens during training.\n",
    "\n",
    "##### Example:\n",
    "If a batch has a maximum length of 10 tokens and a sequence like `\"Hello, world!\"` (with 2 tokens) is in the batch:\n",
    "\n",
    "- **Original**: `\"Hello, world!\"`\n",
    "- **Padded**: `\"<pad>, \"Hello\", \",\", \"world\", \"!\", </s>, <pad>, <pad>, <pad>, <pad>\"`\n",
    "- **Attention Mask**: `[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]`\n",
    "\n",
    "#### Setting Maximum Length\n",
    "By default, the T5-small model expects sequences of up to 512 tokens. However, for this task, we don’t need sequences that long. We’ll set the maximum sequence length to **40 tokens**. This ensures:\n",
    "- Sequences longer than 40 tokens are truncated.\n",
    "- Shorter sequences are padded to reach 40 tokens.\n",
    "\n",
    "Run the following code to configure the tokenizer:"
   ],
   "id": "a7d6c2e291334223"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define maximum sequence length for inputs\n",
    "max_input_length = 40\n",
    "\n",
    "# Set the tokenizer's maximum length\n",
    "tokenizer.model_max_length = max_input_length"
   ],
   "id": "a10bf5bf44b810c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Handling Long Sequences: Truncation\n",
    "\n",
    "When input sequences exceed the specified maximum length, the tokenizer will **truncate** them to fit. Truncation simply means cutting off tokens that exceed the limit, and this can be done either from the start (left truncation) or the end (right truncation) of the sequence.\n",
    "\n",
    "#### Truncation Options\n",
    "- **Right Truncation** (default): Removes tokens from the end of the sequence.\n",
    "- **Left Truncation**: Removes tokens from the start of the sequence.\n",
    "\n",
    "The choice between left or right truncation depends on the task:\n",
    "- For summarization or text generation, keeping the start of the text (left truncation) might not make sense since context at the end is often important.\n",
    "- For tasks like sentence classification, keeping the start (right truncation) may suffice if the relevant information is typically in the first part of the text.\n",
    "\n",
    "#### **Your Turn**:\n",
    "1. Use the cell below to input a long sentence and observe how the tokenizer truncates it.\n",
    "2. Experiment by changing:\n",
    "   - `truncation_side` to `'left'` or `'right'` to see how truncation behaves.\n",
    "   - The `max_input_length` to control the maximum allowed tokens for input sequences."
   ],
   "id": "1f4e7f326d8da862"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set truncation behavior\n",
    "tokenizer.truncation_side = 'right'  # Truncate from the end\n",
    "# tokenizer.truncation_side = 'left'  # Uncomment to truncate from the start\n",
    "\n",
    "# Test the tokenizer with a long sentence\n",
    "show_tokenization(tokenizer, \"write a REALLY long sentence in here and see what happens\")"
   ],
   "id": "f38a256bbc74f640",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Preparing the Dataset for Training\n",
    "\n",
    "Now that we’ve configured the tokenizer, the final step before training is to **preprocess the data**. This involves tokenizing the text messages in our dataset so they are converted into token IDs and ready for the model to process.\n",
    "\n",
    "#### Defining a Preprocessing Function\n",
    "We’ll define a function, `preprocess_function`, to handle this tokenization. The function will:\n",
    "1. Take a batch of examples from the dataset.\n",
    "2. Use the tokenizer to tokenize the `message` field, with truncation enabled to handle sequences longer than the maximum allowed length.\n",
    "\n",
    "Here’s the function:"
   ],
   "id": "502965f3b177a0c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"message\"], truncation=True)"
   ],
   "id": "fffa6578628279e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenizing the Dataset\n",
    "\n",
    "The map method is used to apply the preprocess_function to each batch of examples in the dataset:\n",
    "\n",
    "- Setting `batched=True` ensures that the function processes multiple examples at once, making it more efficient.\n",
    "- The tokenized output will include token IDs and attention masks for each example.\n",
    "\n",
    "Run the following code to tokenize the dataset:"
   ],
   "id": "d7b4f4c7f1e812a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenized_intents = intents.map(preprocess_function, batched=True)",
   "id": "46a2ba13951d2fcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenized_intents['train']",
   "id": "1c5fbcf9894aee67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Collation\n",
    "\n",
    "Before training our model, we need to handle one last preprocessing step: **data collation**. This involves combining individual examples into batches that the model can process efficiently. During collation:\n",
    "- All sequences in a batch are padded to the same length to ensure uniformity.\n",
    "- The resulting batch is formatted into a structure compatible with the model.\n",
    "\n",
    "#### Setting Up a Data Collator\n",
    "We’ll use the `DataCollatorWithPadding` class from the `transformers` library. This class:\n",
    "- Uses the tokenizer to handle padding.\n",
    "- Returns the collated batch as PyTorch tensors (`return_tensors=\"pt\"`).\n"
   ],
   "id": "7dc814511c855efa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Create a data collator for padding and batching\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ],
   "id": "24864c6472656092",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluation Metric\n",
    "\n",
    "During training, we need a way to evaluate the model's performance. We'll compute metrics like **accuracy** to measure how well the model's predictions match the true labels. This will help us monitor progress and identify areas for improvement.\n",
    "\n",
    "#### Setting Up the Metric\n",
    "We'll use the `evaluate` library to calculate accuracy. To do this, we define a function called `compute_metrics`, which processes the model's predictions and computes the desired metric."
   ],
   "id": "328724ff2fc39828"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load the accuracy metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # Unpack predictions and labels\n",
    "    predictions, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "\n",
    "    # Handle tuple predictions (e.g., logits)\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]  # Use the logits\n",
    "\n",
    "    # Convert predictions to a NumPy array if necessary\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Compute class predictions (argmax for classification)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Return the computed accuracy\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ],
   "id": "d2d1cb5dca0d2ea9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Encoding Labels\n",
    "\n",
    "To train our model, we need to convert the **intent labels** into numerical values. Machine learning models work with numbers, not text, so we’ll create a mapping that translates each text label into a unique integer. This process is known as **label encoding**.\n",
    "\n",
    "#### Steps:\n",
    "1. **Create a Mapping**:\n",
    "   - `label2id`: Maps each label (text) to a unique integer.\n",
    "   - `id2label`: Provides the reverse mapping, converting integers back to labels for easy interpretation later.\n",
    "2. **Apply the Mapping**:\n",
    "   - Use the `map` function to encode the `label` field in the dataset, replacing text labels with their corresponding integers."
   ],
   "id": "ba1226cf4465e15a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create mappings from labels to integers and vice versa\n",
    "label2id = {label: i for i, label in enumerate(intents['train'].unique('label'))}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Function to encode labels\n",
    "def encode_label(example):\n",
    "    example['label'] = label2id[example['label']]\n",
    "    return example\n",
    "\n",
    "# Apply the label encoding to the dataset\n",
    "tokenized_intents = tokenized_intents.map(encode_label)\n",
    "\n",
    "# Inspect the encoded training dataset\n",
    "tokenized_intents['train']"
   ],
   "id": "b76a3c69846fb5b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loading the Model\n",
    "\n",
    "With our data preprocessed, it's time to load the **pre-trained model** that we’ll fine-tune for our classification task. Pre-trained models are powerful because they already have a general understanding of language, allowing us to focus on adapting them to our specific problem.\n",
    "\n",
    "#### Model Setup\n",
    "We’ll use the `AutoModelForSequenceClassification` class from the `transformers` library to load a model designed for sequence classification tasks. This class:\n",
    "- Loads a pre-trained model for tasks where the input is a sequence (e.g., text) and the output is a classification label.\n",
    "- Automatically configures the model for the number of classes (27 intent categories in our case).\n",
    "\n",
    "#### Key Parameters:\n",
    "- **Pre-trained Model**: We’re using `t5-small`, a small but capable version of the T5 model, designed for efficient fine-tuning.\n",
    "- **Number of Labels**: `num_labels=27` specifies the number of intent categories in our dataset.\n",
    "- **Label Mappings**: `id2label` and `label2id` ensure that the model can interpret numeric labels and translate them back into text labels when needed.\n",
    "\n",
    "When you run the code to load the model, you’ll likely see a warning message like this:\n",
    "```\n",
    "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "```\n",
    "This message is expected because we haven’t trained the model yet. It’s a reminder that the model’s classification head (the part responsible for classifying sequences) is initialized randomly and needs to be trained on our dataset."
   ],
   "id": "51de3968db58101e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load the pre-trained model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"t5-small\",         # Model name\n",
    "    num_labels=27,      # Number of intent categories\n",
    "    id2label=id2label,  # Mapping from IDs to labels\n",
    "    label2id=label2id   # Mapping from labels to IDs\n",
    ")"
   ],
   "id": "af740a75def25f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "We’re now ready to train our model! The `Trainer` class from the `transformers` library simplifies the training process by handling key tasks like batching, evaluation, and gradient updates. To customize the training, we define a set of **training arguments** that specify how the training should proceed.\n",
    "\n",
    "#### Training Arguments Explained:\n",
    "- **`per_device_train_batch_size`**: The number of samples processed at once on each device (e.g., GPU). Smaller batch sizes may be necessary if memory is limited.\n",
    "- **`per_device_eval_batch_size`**: Similar to the training batch size but used for evaluation.\n",
    "- **`num_train_epochs`**: The number of passes through the entire training dataset.\n",
    "- **`logging_dir`**: Directory where training logs are saved for monitoring.\n",
    "- **`logging_steps`**: How often (in steps) training progress is logged.\n",
    "- **`eval_strategy`**: Specifies when evaluation is performed. Here, it’s set to evaluate every few steps.\n",
    "- **`eval_steps`**: The number of steps between evaluations.\n",
    "- **`save_strategy`**: Determines when the model is saved. Here, saving is disabled with `\"no\"`.\n",
    "- **`output_dir`**: Directory where final model files and outputs will be saved. This is required even if saving is disabled.\n",
    "\n",
    "#### Setting Up the Trainer\n",
    "The `Trainer` class brings everything together:\n",
    "- **Model**: The model to be trained.\n",
    "- **Arguments**: The training configuration (`training_args`).\n",
    "- **Datasets**: The preprocessed training and evaluation datasets.\n",
    "- **Data Collator**: Handles batching and padding during training.\n",
    "- **Metrics**: The `compute_metrics` function for evaluating performance."
   ],
   "id": "4c42d3ff5dcf5c51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,    # Batch size for training\n",
    "    per_device_eval_batch_size=8,    # Batch size for evaluation\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    logging_dir='logs',              # Directory for logs\n",
    "    logging_steps=10,                # Log progress every 10 steps\n",
    "    eval_strategy=\"steps\",           # Evaluate every X steps\n",
    "    eval_steps=10,                   # Perform evaluation every 10 steps\n",
    "    save_strategy=\"no\",              # Disable model saving\n",
    "    output_dir='output'              # Directory for output files\n",
    ")\n",
    "\n",
    "# Set up the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                     # Model to train\n",
    "    args=training_args,              # Training arguments\n",
    "    train_dataset=tokenized_intents['train'],  # Training dataset\n",
    "    eval_dataset=tokenized_intents['test'],    # Evaluation dataset\n",
    "    data_collator=data_collator,     # Handles padding and batching\n",
    "    compute_metrics=compute_metrics  # Evaluates model performance\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ],
   "id": "473c1e3c017258a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "After training, it’s time to evaluate the model's performance on the **test dataset**, which contains unseen examples. This allows us to measure how well the model generalizes to new data.\n",
    "\n",
    "#### Current Performance\n",
    "- After a few epochs of training, the model achieves around **55% accuracy**. While this is a good start, it suggests there’s room for improvement with additional tuning and training.\n",
    "- Remember, with 27 intent categories, random guessing would yield an accuracy of ~3.7%, so 55% represents a significant improvement.\n",
    "\n",
    "#### Evaluation Tools\n",
    "To better understand the model's strengths and weaknesses, we’ll:\n",
    "1. **Generate a Classification Report**:\n",
    "   - The classification report provides metrics like precision, recall, and F1-score for each intent category.\n",
    "   - This helps identify which categories the model performs well on and which are more challenging.\n",
    "2. **Produce a Confusion Matrix**:\n",
    "   - The confusion matrix shows how often each category is confused with others.\n",
    "   - It helps identify specific pairs of intents that the model struggles to distinguish."
   ],
   "id": "64bea5796d38c366"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions on the test dataset\n",
    "predictions = trainer.predict(tokenized_intents['test']).predictions[0]\n",
    "\n",
    "# Compute class predictions\n",
    "class_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get true labels\n",
    "true_labels = tokenized_intents['test']['label']\n",
    "\n",
    "# Convert labels to text\n",
    "true_labels_text = [id2label[label] for label in true_labels]\n",
    "predicted_labels_text = [id2label[label] for label in class_predictions]\n",
    "\n",
    "# Display classification report\n",
    "report = pd.DataFrame(classification_report(true_labels_text, predicted_labels_text, output_dict=True, zero_division=0)).T\n",
    "report"
   ],
   "id": "acfeaa369e82640d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Visualizing the Confusion Matrix\n",
    "\n",
    "The **confusion matrix** provides a detailed view of the model's performance by showing how often each category is predicted correctly or confused with others. It’s a square matrix where:\n",
    "- Rows represent the **true labels**.\n",
    "- Columns represent the **predicted labels**.\n",
    "\n",
    "#### How to Interpret the Confusion Matrix:\n",
    "- Each cell \\((i, j)\\) shows the number of examples with true label \\(i\\) that were predicted as label \\(j\\).\n",
    "- **Diagonal values**: Represent correct predictions (true label matches predicted label).\n",
    "- **Off-diagonal values**: Represent misclassifications, highlighting categories that are commonly confused."
   ],
   "id": "4d61f3ce0bd10cb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels_text, predicted_labels_text)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=id2label.values(), yticklabels=id2label.values())\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "id": "a942e735335ea6f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model in Practice\n",
    "\n",
    "In real-world applications, a model like this can be used to automatically classify customer messages. However, it's often important to account for uncertainty in the model's predictions. For example:\n",
    "- If the model is **confident**, the predicted intent can be used directly.\n",
    "- If the model's **confidence is low**, the message can be flagged for manual review, with the top predicted intents provided to assist human agents.\n",
    "\n",
    "#### Implementing a Practical Prediction Method\n",
    "We’ll write a method, `predict_intent`, that:\n",
    "1. **Classifies Messages**:\n",
    "   - Predicts the most likely intent for each message.\n",
    "2. **Handles Uncertainty**:\n",
    "   - If the model’s confidence (probability of the top prediction) is below a specified threshold, it returns the top 3 most likely intents instead of a single prediction."
   ],
   "id": "31960ec38841b724"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict_intent(model, tokenizer, messages, id2label, threshold=0.5):\n",
    "    # Tokenize the messages\n",
    "    tokenized_messages = tokenizer(messages, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Get model predictions\n",
    "    predictions = model(**tokenized_messages)\n",
    "\n",
    "    # Get logits as numpy array for processing\n",
    "    logits = predictions.logits.detach().cpu().numpy()\n",
    "\n",
    "    # Get predicted class\n",
    "    predicted_class = np.argmax(logits, axis=1).tolist()\n",
    "\n",
    "    # Get predicted probabilities\n",
    "    predicted_probs = np.max(logits, axis=1).tolist()\n",
    "\n",
    "    # Get predicted labels\n",
    "    predicted_labels = [id2label[label] for label in predicted_class]\n",
    "\n",
    "    # Get top intents if confidence is low\n",
    "    if any(prob < threshold for prob in predicted_probs):\n",
    "        top_intents = [\n",
    "            [id2label[i] for i in np.argsort(logit)[::-1][:3]]\n",
    "            for logit in logits\n",
    "        ]\n",
    "        return top_intents[0]\n",
    "    else:\n",
    "        return predicted_labels[0]\n",
    "\n",
    "for message in intents['test']['message'][0:15]:\n",
    "    print(f\"Message: {message}\")\n",
    "    print(f\"Predicted Intent: {predict_intent(model, tokenizer, message, id2label)}\")\n",
    "    print()"
   ],
   "id": "d8115520b6bc91d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Zero-Shot Learning\n",
    "\n",
    "One of the most powerful features of large language models is their ability to perform **zero-shot learning**. Unlike traditional models that require task-specific training, a zero-shot learning model can classify text based on its general understanding of language, even if it hasn’t been explicitly trained on that specific task.\n",
    "\n",
    "#### How It Works:\n",
    "- Instead of fine-tuning the model, you provide it with a **prompt** that describes the task and possible labels (e.g., \"What is the intent of this message?\").\n",
    "- The model uses its pre-trained knowledge to predict the most appropriate label.\n",
    "\n",
    "This approach leverages the model's extensive training on a wide variety of text, making it flexible for many tasks.\n",
    "\n",
    "#### Why Use Zero-Shot Learning?\n",
    "- **Quick Prototyping**: No need to preprocess or fine-tune the model for every new task.\n",
    "- **Versatility**: Works for tasks the model wasn’t explicitly trained on, as long as the task can be described in a prompt.\n",
    "\n",
    "#### Using a Larger Model:\n",
    "For zero-shot classification, we’ll use the `flan-t5-large` model, which is better suited for this task due to its size and broader understanding of language. Since this model doesn’t require fine-tuning, we can focus on testing its performance directly.\n",
    "\n",
    "#### Installation:\n",
    "The `sentencepiece` library is required to use the `flan-t5-large` model. Install it by running the following command:"
   ],
   "id": "f4d4fa07abd72385"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install sentencepiece",
   "id": "c2ff93bb029b3a30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Zero-Shot Intent Classification with Flan-T5\n",
    "\n",
    "We’ll now use the **Flan-T5 large** model to classify intents via zero-shot learning. This approach involves crafting a **prompt** that describes the task and provides the model with the possible labels. The model then uses its language understanding to predict the intent without task-specific training.\n",
    "\n",
    "#### Prompt Construction\n",
    "The prompt is key to zero-shot learning. For our task:\n",
    "1. The prompt begins by instructing the model to classify the intent of the message.\n",
    "2. It lists the available intent categories.\n",
    "3. Finally, it appends the message to classify."
   ],
   "id": "7dba3e9710c2523d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"Classify the intent of the following message using these categories:\\n\"\n",
    "for label in id2label.values():\n",
    "    prompt += f\"- {label}\\n\"\n",
    "prompt += \"Message: \"\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "# Function for zero-shot classification\n",
    "def zero_shot_intent_classification(model, prompt, message):\n",
    "    # Combine the prompt and the message\n",
    "    input_text = prompt + message\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # Generate a prediction\n",
    "    output = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "    # Decode the prediction into text\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ],
   "id": "939de04f2e38e03d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Testing Zero-Shot Intent Classification\n",
    "\n",
    "You can now test the zero-shot classification capabilities of the `flan-t5-large` model on a subset of messages from the test set. This will provide a sense of how well the model performs without task-specific training."
   ],
   "id": "6526c7772ced2e9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for message in intents['test']['message'][25:35]:\n",
    "    print(f\"Message: {message}\")\n",
    "    print(f\"Predicted Intent: {zero_shot_intent_classification(model, prompt, message)}\")\n",
    "    print()"
   ],
   "id": "ac91494295609a37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluating Zero-Shot Model Performance on the Test Dataset\n",
    "\n",
    "To evaluate the performance of the `flan-t5-large` zero-shot model on the entire test dataset, we’ll:\n",
    "1. **Generate Predictions**: Use the `zero_shot_intent_classification` function to predict intents for all test messages.\n",
    "2. **Compare Predictions**: Compare the zero-shot predictions to the true labels in the test set.\n",
    "3. **Generate a Classification Report**: Use `classification_report` to compute metrics such as precision, recall, and F1-score for each intent category."
   ],
   "id": "a3121167999eefa2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Generate predictions for the test dataset using the zero-shot model\n",
    "zero_shot_predictions = [\n",
    "    zero_shot_intent_classification(model, prompt, message)\n",
    "    for message in tqdm(intents['test']['message'])\n",
    "]\n",
    "\n",
    "# Create a classification report\n",
    "zero_shot_report = pd.DataFrame(\n",
    "    classification_report(true_labels_text, zero_shot_predictions, output_dict=True, zero_division=0)\n",
    ").T\n",
    "\n",
    "# Display the report\n",
    "zero_shot_report"
   ],
   "id": "92119156ba498ead",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Incredibly, our accuracy using zero-shot learning is around **70%**, which is significantly higher than our fine-tuned model! This demonstrates the power of large language models and their ability to generalize to new tasks without explicit training.\n",
   "id": "e268670fc0f22457"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "zero_shot_conf_matrix = confusion_matrix(true_labels_text, zero_shot_predictions)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(zero_shot_conf_matrix, annot=True, fmt='d', xticklabels=id2label.values(), yticklabels=id2label.values())\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Zero-Shot Confusion Matrix')\n",
    "plt.show()"
   ],
   "id": "d85e8b5003268666",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "In this lab, you explored how to classify customer text messages into different intent categories using both a **fine-tuned model** and a **zero-shot learning approach**. Along the way, you learned about key concepts in NLP and machine learning, such as tokenization, data preprocessing, and evaluation metrics.\n",
    "\n",
    "#### Key Takeaways:\n",
    "1. **Fine-Tuning with Transformers**:\n",
    "   - You fine-tuned the `T5-small` model for intent classification.\n",
    "   - By adapting a pre-trained model, you achieved an accuracy of around **55%** after training, which is a significant improvement over random guessing (~3.7% accuracy for 27 classes).\n",
    "\n",
    "2. **Understanding Model Predictions**:\n",
    "   - You evaluated the model’s predictions using metrics like precision, recall, and F1-score, and visualized the results with a confusion matrix.\n",
    "   - You implemented a method to handle uncertainty, ensuring the model flagged low-confidence predictions for manual review.\n",
    "\n",
    "3. **Zero-Shot Learning**:\n",
    "   - Using the `flan-t5-large` model, you explored the power of zero-shot learning, achieving an impressive accuracy of around **70%** without any task-specific training.\n",
    "   - This highlighted the flexibility and capability of large language models to generalize across tasks.\n",
    "\n",
    "#### Real-World Implications:\n",
    "- The methods you practiced in this lab can be applied to automate tasks like customer service classification, intent detection in chatbots, or filtering messages for human review.\n",
    "- Zero-shot learning offers a quick way to prototype solutions when labeled data is scarce, while fine-tuning allows for highly customized and accurate models with enough training data."
   ],
   "id": "736af25538271203"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
