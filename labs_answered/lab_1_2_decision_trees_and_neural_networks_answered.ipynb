{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SF0pPVB4-LHp"
   },
   "source": [
    "# AI Workshop - Lab 2-1: Decision Trees and Neural Networks\n",
    "\n",
    "In the first lab, we used a decision tree classifier to evaluate the quality of different imputation methods, but we didn't really discuss what decision trees are or how they work. In this lab, we will delve into the details of decision trees and explore how they can be used to make predictions with our energy and weather datasets."
   ]
  },
  {
   "metadata": {
    "id": "DB7wUAGFMwVi"
   },
   "cell_type": "code",
   "source": [
    "# Install graphviz, which we will use later in the lab\n",
    "!pip install -Uq graphviz"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7CedHdx-LHw"
   },
   "source": [
    "### Decision Trees\n",
    "\n",
    "Decision trees are popular supervised learning methods used for classification and regression. The tree represents a series of simple decision rules that predict the target when the feature vector is passed through them. Decision trees are easy to understand, can be visualized nicely, require very little data preparation (e.g., we don't need to scale features), and the trained model can be explained easily to others post priori (as opposed to other *black box* methods that are difficult to communicate).\n",
    "\n",
    "###### Example\n",
    "Suppose you wanted to design a simple decision tree for whether (or not) you buy a used car. You might develop something like the following:\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/decision-tree.gif?raw=1\" width=\"500\"/>\n",
    "\n",
    "**YOUR TURN:** Let's say you're browsing Kijiji and come across a used car that: has been road tested, has high mileage, and is a recent year/model.\n",
    "* According to your decision tree model, should you buy this car or not? ____________________________\n",
    "* Will you buy any cars that haven't been road tested (if you follow your model)? ___________________________________\n",
    "\n",
    "Obviously this tree may not be ideal, depending on the situation. For example, you could have a road tested car of a recent year with 2,000,000 km's on it and the model is telling you to buy! (But, you probably shouldn't)\n",
    "\n",
    "### Energy and Weather Datasets\n",
    "\n",
    "Just as in the first lab, we are going to use the energy and weather datasets to explore decision trees. To save time, we have prepared a cleaned version that we will use this afternoon. Let's load the data and take a look at the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mDl1JKwx-LHx",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687
    },
    "outputId": "568c744b-f3f1-4a88-fa1c-8b4be9a2bbd2"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/cleaned_energy_data.csv')\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "BRObUOh0MwVk"
   },
   "cell_type": "markdown",
   "source": [
    "As we have seen already, we can use Pandas to conveniently summarize key aspects of the data. For example, we can use the `describe` method to get a quick statistical summary of the data:\n"
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "uVAoK4nNMwVl",
    "outputId": "1d16d4c0-af71-4b96-bff5-cf97389bc260"
   },
   "cell_type": "code",
   "source": [
    "df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-a-uiu0-LH3"
   },
   "source": [
    "##### Model Development\n",
    "\n",
    "In the previous lab, we looked at predicting weather conditions based on energy demand. We found that there are too many types of condition for a simple model to predict accurately. This time, we'll look specifically at the presence or absence of clouds using the `clouds_all` column (which records the percentage cloud cover at the time of the observation).\n",
    "\n",
    "Let's import sklearn's decision tree classifer and split the data (using techniques we covered in the first lab)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uXuxpDkP-LH3"
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target_data = df['clouds_all'] != 0\n",
    "feature_data = df[['generation biomass', 'generation fossil brown coal/lignite',\n",
    "                   'generation fossil gas',\n",
    "                   'generation fossil hard coal', 'generation fossil oil',\n",
    "                   'generation hydro pumped storage consumption',\n",
    "                   'generation hydro run-of-river and poundage',\n",
    "                   'generation hydro water reservoir',\n",
    "                   'generation nuclear', 'generation other', 'generation other renewable',\n",
    "                   'generation solar', 'generation waste',\n",
    "                   'generation wind onshore']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_data, target_data, test_size=0.3, random_state=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4U5WwH2-LH4"
   },
   "source": [
    "**YOUR TURN:**\n",
    "* How many samples are in the training set? _______________________\n",
    "* How many samples are in the test set? _______________________\n",
    "* What percentage of the samples have clouds in the training set? _______________________"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ANVOauwm-LH4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d4716864-c617-461a-df87-c5c4b84c25ed"
   },
   "source": [
    "## Your code here\n",
    "print(f'Training samples: {len(X_train)}')\n",
    "print(f'Test samples: {len(X_test)}')\n",
    "print(f'Percentage of samples with clouds in the training set: {y_train.mean()}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZp4c8Hg-LH5"
   },
   "source": [
    "##### Dealing with Missing Data: Imputation\n",
    "\n",
    "Before we can fit our decision tree to our training data, we can conduct imputation to replace missing values in our dataset. Previously we did this manually so that we could follow the process, but now we'll use the handy `SimpleImputer` class from sklearn. This class allows us to replace missing values with a specified strategy (e.g., mean, median, most frequent)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fQFGk9hk-LH5"
   },
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "imp.fit(X_train)\n",
    "X_train = imp.transform(X_train) # replace missing data using our imputer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqj-hPeg-LH6"
   },
   "source": [
    "So we've got our data prepared, let's fit a decision tree to our training data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VqxAAXVU-LH6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5f0e54fe-3e32-4fde-84fa-3bc614604ca4"
   },
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=10) # We'll set a max depth to prevent overfitting\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "accuracy = accuracy_score(y_train, clf.predict(X_train))\n",
    "print (\"Accuracy: \", accuracy * 100, \"%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1yMNmyr-LH6"
   },
   "source": [
    "In the above cell, we defined a Decision Tree classifier and fit it to our training set. When we then used it to predict training set values, the resulting accuracy was ~63%\n",
    "\n",
    "**YOUR TURN:**\n",
    "* Since we are both training and predicting on our training set, why didn't the decision tree achieve 100% accuracy?\n",
    "* What is the performance of this model on the test set?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F4vftzMR-LH7"
   },
   "source": [
    "## Your code here\n",
    "##\n",
    "##\n",
    "##"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4KZ5J-t-LH7"
   },
   "source": [
    "##### Feature Importances\n",
    "\n",
    "One thing we can do is take a look at the relative feature importances of the trained decision tree classifier. This will give us an idea of what the model thinks is more/less important for properly predicting the target.\n",
    "\n",
    "Let's look at the feature importances for a model on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w-1ZcqEN-LH7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "141dc03f-c6e0-4cbb-8fca-a596cb08f94b"
   },
   "source": [
    "for feature, importance in zip(feature_data.columns, clf.feature_importances_):\n",
    "    print(f'{feature:<43} {importance:0.2f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HL_BERf-LH8"
   },
   "source": [
    "As we can see, the model places a higher importance on onshore nuclear generation than other features, although the importance is broadly spread across all features.\n",
    "\n",
    "#### Visualizing the Tree\n",
    "\n",
    "One useful thing we can do is actually visualize our decision tree model! We can use the [graphViz](https://www.graphviz.org/) library to accomplish this:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nQ2xJfJm-LH8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "3bec6df6-ed28-458a-87d3-70d8a545bfd0"
   },
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz # Package containing visualization tools\n",
    "\n",
    "export_graphviz(clf, out_file=\"mytree.dot\", feature_names=feature_data.columns) # export the tree to .dot file\n",
    "with open(\"mytree.dot\") as f: # read the file back in\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph) # display the tree"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EgolEWB-LH8"
   },
   "source": [
    "**YOUR TURN:** Explore the decision tree and answer the following:\n",
    "* What feature does the root node split on?\n",
    "* What is the depth of the decision tree (i.e., the length of the longest path from root to leaf)?\n",
    "* Do you think this decision tree is prone to overfitting? Why/why not?\n",
    "\n",
    "To reduce the degree to which this tree is overfit to the training data, we can force the tree to be of some *maximum depth*. This ensures the tree won't be able to just keep generating new layers to properly classify every sample in the training stage (and, thus, presumably generalize better to the test set).\n",
    "\n",
    "Let's try limiting the max depth to 2 and visualizing the resulting tree."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4uDkSy0a-LH9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "outputId": "3b04b557-d7d7-4b1f-a3b3-9e7ca78c4c7f"
   },
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth = 2)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "export_graphviz(clf, out_file=\"mytree.dot\", feature_names=feature_data.columns)\n",
    "with open(\"mytree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUJdXnp--LH9"
   },
   "source": [
    "Much simpler!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are many hyper-parameters that can be tuned to change how the model performs. Some common parameters that are modified include:\n",
    "1. Max Tree Depth: How \"tall\" do you want your tree to be\n",
    "2. Minimum Samples Per Leaf: This parameter defines the minimum number of training datapoints that fall into a given leaf node in order for that node to be created\n",
    "3. Minimum Samples to Split: This parameter controls the minimum number of samples required to create a decision split\n",
    "\n",
    "To decide the values of each of the parameters, we can use Grid Search combined with cross validation. In Grid Search, we first decide what potential values we want each hyperparameter will take. Then we find every possible combination of parameters and run cross validation on each combination to estimate the performance of that hyperparameter combination.\n",
    "\n",
    "Luckily, `sklearn` has a nice implementation of Grid Search that runs this algorithm for us. Lets see a demo below:"
   ],
   "metadata": {
    "id": "T2gqE8y3_yXP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "clf = tree.DecisionTreeClassifier() # First we define our model without passing in parameters\n",
    "hyperparameter_search = { # Then we decide the possible parameter combinations\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 8, 11],\n",
    "    'min_samples_leaf': [2, 5, 8, 11]\n",
    "} # Since we have 3 parameters with 2 possible values, grid search will test 3^3 combinations\n",
    "evaluation_metric = make_scorer(accuracy_score, # GridSearchCV requires us to wrap our metric function in a \"scorer\"\n",
    "                                greater_is_better = True)\n",
    "\n",
    "grid_search_cv = GridSearchCV(estimator = clf,\n",
    "                              param_grid = hyperparameter_search,\n",
    "                              scoring = evaluation_metric,\n",
    "                              n_jobs = -1, # Use all available cores\n",
    "                              cv = 5) # Set up search algorithm\n",
    "grid_search_cv.fit(X_train, y_train) # Run the search. NOTE: This may take a while\n",
    "\n",
    "print(\"Best Parameters: \", grid_search_cv.best_params_) # Print the parameters\n",
    "print(f'Best Accuracy: {grid_search_cv.best_score_ * 100:0.2f}%') # Print the accuracy of the best model\n",
    "\n",
    "clf = grid_search_cv.best_estimator_ # Get the best model from the GridSearch\n",
    "accuracy = accuracy_score(y_test, clf.predict(imp.transform(X_test)))\n",
    "print(f'Accuracy on test set: {accuracy * 100:0.2f}%')"
   ],
   "metadata": {
    "id": "zmpcYAMI_xyX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2732628d-480b-4b54-fcd7-91504abfea38"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the cell above, we tested our two values per hyperparameter and ran grid search to find the best combination from the space we defined. As you may have noticed, the number of combinations tested by Grid Search exponentially increases as you test more values and tune more hyperparameters. This means that performing a grid search is often a task that takes a long period of time and is often note used for more complex models like neural networks."
   ],
   "metadata": {
    "id": "Qrr1hd59EiVK"
   }
  },
  {
   "metadata": {
    "id": "--drxZvxMwVq"
   },
   "cell_type": "markdown",
   "source": [
    "# Neural Networks with Keras\n",
    "\n",
    "Let's move on to making predictions on our data using neural networks. We will use the Keras library to build a simple neural network and evaluate its performance on the test set.\n",
    "\n",
    "This time, we will use our data for a *regression* task instead of a classification task. We will predict the amount of energy consumed based on the weather conditions (perhaps more in line with what you were expecting all along!) We will use a simple feedforward neural network with one hidden layer.\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "Let's prepare our X and y data for this new task. We will predict the total energy consumed based on the weather conditions."
   ]
  },
  {
   "metadata": {
    "id": "e4erskPOMwVr"
   },
   "cell_type": "code",
   "source": [
    "target_data = df['total load actual']\n",
    "\n",
    "feature_data = df[['city_name', 'temp', 'temp_min', 'temp_max', 'pressure',\n",
    "                   'humidity', 'wind_speed', 'wind_deg', 'rain_1h', 'rain_3h', 'snow_3h',\n",
    "                   'clouds_all', 'weather_main']]\n",
    "\n",
    "# One-hot encode city_name, weather_main\n",
    "feature_data = pd.get_dummies(feature_data, columns=['city_name', 'weather_main'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "aoAjJCdGMwVr"
   },
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature_data, target_data, test_size=0.3, random_state=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "euN0UIHsMwVr"
   },
   "cell_type": "code",
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "imp.fit(X_train)\n",
    "X_train = imp.transform(X_train) # replace missing data using our imputer\n",
    "X_test = imp.transform(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "ZeBH5Rm4MwVr"
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "-bt6Ojz8MwVr"
   },
   "cell_type": "markdown",
   "source": [
    "Let's check our data to confirm it's what we expect."
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "fDfnzOHCMwVs",
    "outputId": "c589eb0d-3698-4e2c-89c6-1273903155ac"
   },
   "cell_type": "code",
   "source": [
    "pd.DataFrame(X_train, columns=feature_data.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "acEjCJwaMwVs",
    "outputId": "9fb7b204-600c-41ea-ff5d-13247c53f11e"
   },
   "cell_type": "code",
   "source": [
    "y_train"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "2chjs40LMwVs"
   },
   "cell_type": "markdown",
   "source": [
    "Looks good - we have our X and y data prepared for the regression task. Now we can build our neural network model!\n",
    "\n",
    "## Building a Neural Network with Keras\n",
    "\n",
    "Keras is a high-level neural networks API, written in Python and capable of running TensorFlow code. We will use Keras to build a simple feedforward neural network with one hidden layer. We will use the `Sequential` model from Keras to build our network.\n",
    "\n",
    "Let's start by importing the necessary modules from Keras, and recording some of the data shapes (specifically, the number of features in our input data)."
   ]
  },
  {
   "metadata": {
    "id": "h8uTl6YLMwVs"
   },
   "cell_type": "code",
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input\n",
    "\n",
    "input_shape = X_train.shape[1]\n",
    "output_shape = 1 # We are predicting a single value"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "mpWcs3LtMwVs"
   },
   "cell_type": "markdown",
   "source": [
    "Now we can build our model. We will use the `Sequential` model from Keras to build a simple feedforward neural network with one hidden layer. We will use the `Dense` class to define the layers in our network, which are fully connected layers.\n",
    "\n",
    "Let's start by building a simple model with 1 hidden layer and 10 neurons. We need to make sure that the input shape of the first layer matches the number of features in our input data."
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "yUL41VlFMwVs",
    "outputId": "058a910d-fda2-43c8-f466-93294b0277e8"
   },
   "cell_type": "code",
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Input(shape=(input_shape,)), # Defining the input layer\n",
    "        Dense(10), # <--- Hidden layer with 10 neurons\n",
    "        Activation('relu'), # Simple ReLU activation\n",
    "        Dense(output_shape) # One output neuron\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(optimizer='sgd', loss='mean_absolute_error') # Compile the model\n",
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "3zd4aFBnMwVt"
   },
   "cell_type": "markdown",
   "source": [
    "We have built a simple neural network with 1 hidden layer and 10 neurons. The model has 301 parameters (weights) that need to be learned.\n",
    "\n",
    "We defined our network to be trained using Stochastic Gradient Descent, the most basic optimization algorithm. We also defined the loss function to be the mean absolute error, which is commonly used for regression tasks.\n",
    "\n",
    "Now we are ready to train our model! We will use the `fit` method to train the model on our training data."
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNCLpP72MwVt",
    "outputId": "87386f24-5f53-4992-cb08-8beb4f9e4c76"
   },
   "cell_type": "code",
   "source": [
    "model.fit(\n",
    "    X_train, y_train, # Training data\n",
    "    epochs=5, # Number of epochs to train for\n",
    "    batch_size=128, # Number of samples per gradient update\n",
    "    validation_data=(X_test, y_test) # Validation data\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a final check, let's calculate both the Mean Absolute Error and the Mean Absolute Percentage Error on the testing set. As it sounds, the MAPE gives us the average percentage that the predictions differ from the true value, which is helpful for understanding the level of accuracy."
   ],
   "metadata": {
    "id": "1R0BqQP3Q6BG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'MAE:  {mean_absolute_error(y_test, y_pred):.02f}')\n",
    "print(f'MAPE: {mean_absolute_percentage_error(y_test, y_pred)*100:.02f}%')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-QAHIrgQdTX",
    "outputId": "2b5ad974-5822-40a8-9a06-f9d623bdd395"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "A9X9pcMeQuRj"
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
