{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AI Workshop - Lab 2-2: Intent Classification\n",
    "\n",
    "In this lab, we’ll build a system to classify customer text messages into different categories (called **intents**) using a powerful type of AI model called a transformer. Transformers are a key technology behind tools like ChatGPT and other modern language systems, but don’t worry if you’re new to them—we’ll break it down step by step.\n",
    "\n",
    "### Data Overview\n",
    "\n",
    "We’re working with a dataset of customer text messages that has already been labeled with their intent (e.g., \"Order Status\", \"Product Inquiry\", \"Account Help\"). The goal is to teach the model to recognize these patterns so it can classify new messages correctly.\n",
    "\n",
    "- **Dataset**:\n",
    "  - Provided as two files: one for training and one for testing.\n",
    "  - Training data is used to teach the model, and testing data is used to see how well it learned.\n",
    "- **Number of Categories**: 27 different intents.\n",
    "\n",
    "### What We’ll Do in This Lab\n",
    "1. **Load the Data**:\n",
    "   - Open and inspect the dataset to understand its structure.\n",
    "   - Check how many examples we have for each intent.\n",
    "2. **Prepare the Data**:\n",
    "   - Use a tool called a **tokenizer** to break down text messages into a format the model can understand.\n",
    "   - Convert the intent labels into numbers so the model can learn from them.\n",
    "3. **Use a Pre-Trained Model**:\n",
    "   - Start with an existing model called `T5-small` that already knows a lot about language.\n",
    "   - Customize (or fine-tune) it to focus on the intents in our dataset.\n",
    "4. **Train the Model**:\n",
    "   - Use the prepared data to train the model step by step.\n",
    "   - Measure how well it’s doing along the way.\n",
    "5. **Evaluate the Model**:\n",
    "   - Test the model on new data it hasn’t seen before.\n",
    "   - Check how accurate it is and where it might make mistakes.\n",
    "\n",
    "### What You’ll Learn\n",
    "- **Transformers**: Get an introduction to these models and why they’re so powerful for language tasks.\n",
    "- **Fine-Tuning**: Learn how to take a pre-trained model and adapt it to solve a specific problem.\n",
    "- **Model Evaluation**: Understand how to measure a model’s performance and interpret its predictions.\n",
    "\n",
    "### HuggingFace Libraries\n",
    "\n",
    "So far we have been working with Keras, a popular library for building neural networks. In this lab, we’ll use the HuggingFace libraries, which are designed specifically for working with transformers.\n",
    "\n",
    "The main HuggingFace library is called `transformers`, and it provides tools for working with pre-trained models, tokenizers, and training pipelines. We’ll also use `datasets` to load and process our data. `accelerate` and `evaluate` are additional libraries that help speed up training and evaluate models, respectively. Install them below:"
   ],
   "id": "89800ddaafaa2296"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:20:56.243917Z",
     "start_time": "2024-12-02T19:20:54.109389Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -Uq datasets transformers accelerate evaluate",
   "id": "3d3c731f7713c24a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For this lab, it's essential that we have a GPU available to speed up training. On Google Colab, you can enable a GPU by going to **Runtime** > **Change runtime type** > **Hardware accelerator** > **GPU**.\n",
    "\n",
    "The following line of code will check if a GPU is available:"
   ],
   "id": "1bcffd8486a0820c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:28:03.363531Z",
     "start_time": "2024-12-02T19:28:03.360417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print('GPU is available!')\n",
    "else:\n",
    "    print('GPU is not available. Enable a GPU runtime in Colab under \"Runtime\" > \"Change runtime type\".')"
   ],
   "id": "967b287ee6c7830c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Loading the Dataset\n",
    "\n",
    "Great! Now that we have our packages installed and imported, we can get going with loading the dataset.\n",
    "\n",
    "We will be working with a dataset of customer text messages that have been labeled with their intent. Let's load the dataset and inspect it to understand its structure."
   ],
   "id": "b3a8dbd2fdcd2d6b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-02T19:28:37.518397Z",
     "start_time": "2024-12-02T19:28:37.516169Z"
    }
   },
   "source": "from datasets import load_dataset",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:28:38.551600Z",
     "start_time": "2024-12-02T19:28:37.854767Z"
    }
   },
   "cell_type": "code",
   "source": "intents = load_dataset(\"alexwaolson/customer-intents\")",
   "id": "7d83bae93beee534",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:28:38.563065Z",
     "start_time": "2024-12-02T19:28:38.559700Z"
    }
   },
   "cell_type": "code",
   "source": "intents['train']",
   "id": "299346eb1c241001",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['message', 'label'],\n",
       "    num_rows: 1555\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you can see, the dataset is comprised of `message` and `label` columns. The `message` column contains the text of the customer message, and the `label` column contains the intent category. There are 27 possible intent categories in this dataset. We can count how many examples we have for each intent to see if the dataset is balanced.",
   "id": "9b2a9381462eaa7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:29:08.938933Z",
     "start_time": "2024-12-02T19:29:08.931003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(intents['train']['label'])"
   ],
   "id": "4b11c9f3d9801d3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'edit account': 65,\n",
       "         'delivery period': 65,\n",
       "         'get refund': 63,\n",
       "         'check payment methods': 62,\n",
       "         'change shipping address': 62,\n",
       "         'check cancellation fee': 62,\n",
       "         'check invoice': 61,\n",
       "         'payment issue': 60,\n",
       "         'set up shipping address': 59,\n",
       "         'create account': 58,\n",
       "         'track refund': 58,\n",
       "         'complaint': 58,\n",
       "         'contact customer service': 58,\n",
       "         'change order': 58,\n",
       "         'switch account': 57,\n",
       "         'cancel order': 57,\n",
       "         'get invoice': 56,\n",
       "         'track order': 56,\n",
       "         'newsletter subscription': 56,\n",
       "         'recover password': 55,\n",
       "         'delete account': 54,\n",
       "         'delivery options': 54,\n",
       "         'place order': 53,\n",
       "         'check refund policy': 53,\n",
       "         'contact human agent': 52,\n",
       "         'registration problems': 52,\n",
       "         'review': 51})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocessing the Dataset\n",
    "\n",
    "Now that we have our dataset loaded, we need to preprocess it for training. This involves tokenizing the inputs and outputs, padding them to a fixed length, and setting up the data collator for training. We'll walk through each of these steps together in the next few cells.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Almost all language models don't work directly with text but with _tokenized_ inputs. Tokenization is the process of splitting a text into individual words, subwords, or characters, which are then mapped to unique IDs (integers) by the model's tokenizer. This is then a format that can slot directly into the linear algebra underpinning any deep learning model.\n",
    "\n",
    "For most LLMs, there are tokens corresponding to most words you'd expect to find, but also for things like common suffixes and prefixes. This allows the model to generalize better to new words that it hasn't seen before. For example, the word \"running\" might be tokenized into \"run\" and \"##ning\" (the \"##\" prefix indicates that the token is a suffix). This allows the model to learn separately the meaning of the word \"run\" and the suffix \"-ning\", then combine them to understand the word \"running\".\n",
    "\n",
    "We'll use the `AutoTokenizer` class from the `transformers` library to load a tokenizer that matches the model we're using. In this case, we'll use the `t5-small` model, which is a smaller version of the T5 model developed by Google. T5 stands for \"Text-to-Text Transfer Transformer\", and it's a versatile model that can be fine-tuned for many different NLP tasks."
   ],
   "id": "88b3c3de6ae442f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:30:56.518251Z",
     "start_time": "2024-12-02T19:30:56.388002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load our tokenizer\n",
    "model_name = 't5-small'\n",
    "# The AutoTokenizer class will automatically select the correct tokenizer class for the model!\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "id": "c1d73e9530267d08",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's inspect the tokenizer a bit more closely to understand what it's capable of. First, we can look at the vocabulary size and the special tokens that the tokenizer uses. The special tokens are used to mark the beginning and end of sequences, as well as to pad sequences to a fixed length.",
   "id": "23d56975de7a7578"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f'Vocab size: {tokenizer.vocab_size}')\n",
    "print(f'Special tokens: {tokenizer.special_tokens_map}')"
   ],
   "id": "96ef738b67675e92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can also see how the tokenizer encodes and decodes text. The `encode` method takes a string and converts it into a list of token IDs, while the `decode` method takes a list of token IDs and converts it back into a string. Let's write a method that shows us what a given input looks like encoded, as well as when we translate that back into text.\n",
    "\n",
    "**Your Turn**: Write a sentence in the `show_tokenization` function and see how it gets tokenized by the model. Try inserting a sentence containing a made up word, or a word that you think might be tokenized into multiple tokens."
   ],
   "id": "d31cbc545d9c61ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:31:29.669206Z",
     "start_time": "2024-12-02T19:31:29.664383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def show_tokenization(tokenizer, text):\n",
    "    print(f'Original text: {text}')\n",
    "    tokens = tokenizer(text, truncation=True)['input_ids']\n",
    "    for token in tokens:\n",
    "        print(f'{tokenizer.decode([token]):10} -> {token}')\n",
    "\n",
    "# Write any sentence and see how it gets tokenized by the model:\n",
    "show_tokenization(tokenizer, 'your sentence here')"
   ],
   "id": "33041b43d6be5ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: your sentence here\n",
      "your       -> 39\n",
      "sentence   -> 7142\n",
      "here       -> 270\n",
      "</s>       -> 1\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Hopefully you can see how the tokenizer works now! We also glossed over a term which you may spot later on: the `attention_mask`. This is a vector that tells the model which tokens are part of the input and which are left over at the end (padding tokens).\n",
    "\n",
    "### Padding\n",
    "\n",
    "When training a model, it's common to train on batches of data. However, each sequence in a batch needs to be the same length. This is where padding comes in: we add special padding tokens to the end of sequences that are shorter than the maximum length in the batch. This ensures that all sequences are the same length and can be processed in parallel.\n",
    "\n",
    "For example, if we wanted all of our batches to have 20 tokens, and we put in the string \"Hello, world!\", we would translate that into five tokens (don't forget that there's punctuation and an end of string). We would then pad the rest of the sequence with padding tokens until we reach 20 tokens.\n",
    "\n",
    "Our T5-small model by default expects sequences of 512 tokens. This is a lot, and we don't need that many for our task. We'll set the maximum length to 40 tokens for the input. This means that any sequence longer than 40 tokens will be truncated to 40 tokens."
   ],
   "id": "a7d6c2e291334223"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define maximum sequence lengths\n",
    "max_input_length = 40\n",
    "\n",
    "tokenizer.model_max_length = max_input_length"
   ],
   "id": "a10bf5bf44b810c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So what happens if we input a sequence that's _longer_ than the maximum length? The tokenizer will _truncate_ the sequence to the max length, which really just means chopping off any excess. Depending on the task at hand, this can either be done by cutting the start (left truncation) or the end (right truncation) of the sequence.\n",
    "\n",
    "**Your Turn**: Try changing the `truncation_side` in the cell below to see how the tokenizer behaves when you input a long sentence. You can also try changing the `max_input_length` and `max_target_length` to see how the tokenizer behaves when you input a sentence that's longer than the maximum length."
   ],
   "id": "1f4e7f326d8da862"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer.truncation_side = 'right'  # Truncate from the right side, i.e. the end of the sequence\n",
    "# tokenizer.truncation_side = 'left'  # Truncate from the left side, i.e. the start of the sequence\n",
    "\n",
    "show_tokenization(tokenizer, \"write a REALLY long sentence in here and see what happens\")"
   ],
   "id": "f38a256bbc74f640"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With all this set up, we are nearly ready to pre-process our data. The last step is to define a function that will take in a batch of examples and tokenize them. This function will be used to process our dataset before training.",
   "id": "502965f3b177a0c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:32:25.002337Z",
     "start_time": "2024-12-02T19:32:24.997169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"message\"], truncation=True)"
   ],
   "id": "fffa6578628279e9",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:32:25.499123Z",
     "start_time": "2024-12-02T19:32:25.480027Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_intents = intents.map(preprocess_function, batched=True)",
   "id": "46a2ba13951d2fcc",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:32:26.304950Z",
     "start_time": "2024-12-02T19:32:26.300776Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_intents['train']",
   "id": "1c5fbcf9894aee67",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['message', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1555\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Collation\n",
    "\n",
    "We have a few small steps left before we can start training our model. One of these is to set up a data collator. This is a function that takes a list of examples and collates them into a batch that can be fed into the model. The data collator will also ensure that the input sequences are padded to the same length."
   ],
   "id": "7dc814511c855efa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:32:27.923453Z",
     "start_time": "2024-12-02T19:32:27.921054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ],
   "id": "24864c6472656092",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluation Metric\n",
    "\n",
    "When we train the model, the quality of our predictions will be fed into the training loop. We can use this to compute metrics like accuracy, which tells us how often the model's predictions match the true labels."
   ],
   "id": "328724ff2fc39828"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:21:16.657956Z",
     "start_time": "2024-12-02T19:21:15.582767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ],
   "id": "bcb2ca52ef98dd4b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:21:16.668256Z",
     "start_time": "2024-12-02T19:21:16.664645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # Unpack the predictions and labels\n",
    "    predictions, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "\n",
    "    # Handle tuple predictions\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]  # Take the first element, assuming it's the logits\n",
    "\n",
    "    # Convert to NumPy array if necessary\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Compute class predictions\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Return computed metrics\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n"
   ],
   "id": "d2d1cb5dca0d2ea9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Encoding Labels\n",
    "\n",
    "Before we can train our model, we need to convert the intent labels into numbers. This is because the model can't learn from text labels directly—it needs numbers. We'll create a mapping from the text labels to numbers, and then use this mapping to convert the labels in our dataset."
   ],
   "id": "ba1226cf4465e15a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:21:16.768483Z",
     "start_time": "2024-12-02T19:21:16.752749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert labels to integers\n",
    "label2id = {label: i for i, label in enumerate(intents['train'].unique('label'))}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "def encode_label(example):\n",
    "    example['label'] = label2id[example['label']]\n",
    "    return example\n",
    "\n",
    "tokenized_intents = tokenized_intents.map(encode_label)"
   ],
   "id": "b76a3c69846fb5b6",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:21:16.850443Z",
     "start_time": "2024-12-02T19:21:16.844759Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_intents['train']",
   "id": "600bb1d54469c32b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['message', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1555\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Loading the model\n",
    "\n",
    "Now that we've preprocessed our data, we can load the pre-trained model that we'll be fine-tuning. We'll use the `AutoModelForSequenceClassification` class from the `transformers` library to load a model that's already set up for sequence classification."
   ],
   "id": "51de3968db58101e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:21:17.359622Z",
     "start_time": "2024-12-02T19:21:16.927419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"t5-small\",\n",
    "    num_labels=27,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ],
   "id": "af740a75def25f6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "Finally, we're ready to start training our model! We'll use the `Trainer` class from the `transformers` library to handle the training process. We'll also define some training arguments, like the number of epochs, the batch size, and the learning rate. The purpose behind each of them is explained below."
   ],
   "id": "4c42d3ff5dcf5c51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:21:51.510317Z",
     "start_time": "2024-12-02T19:21:50.405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    output_dir='output'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_intents['train'],\n",
    "    eval_dataset=tokenized_intents['test'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "473c1e3c017258a8",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:22:58.979131Z",
     "start_time": "2024-12-02T19:21:52.002197Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "e0d9f665319dec61",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='585' max='585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [585/585 01:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.375800</td>\n",
       "      <td>3.307129</td>\n",
       "      <td>0.061697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.347900</td>\n",
       "      <td>3.284916</td>\n",
       "      <td>0.074550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.296900</td>\n",
       "      <td>3.266773</td>\n",
       "      <td>0.079692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.334000</td>\n",
       "      <td>3.250174</td>\n",
       "      <td>0.100257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.345600</td>\n",
       "      <td>3.234811</td>\n",
       "      <td>0.125964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.271500</td>\n",
       "      <td>3.222997</td>\n",
       "      <td>0.125964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.282300</td>\n",
       "      <td>3.214721</td>\n",
       "      <td>0.120823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.232200</td>\n",
       "      <td>3.203785</td>\n",
       "      <td>0.118252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.250700</td>\n",
       "      <td>3.191883</td>\n",
       "      <td>0.113111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.231400</td>\n",
       "      <td>3.177467</td>\n",
       "      <td>0.118252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.200100</td>\n",
       "      <td>3.157994</td>\n",
       "      <td>0.164524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.149900</td>\n",
       "      <td>3.140149</td>\n",
       "      <td>0.167095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.211000</td>\n",
       "      <td>3.122959</td>\n",
       "      <td>0.177378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.203600</td>\n",
       "      <td>3.101764</td>\n",
       "      <td>0.190231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.208200</td>\n",
       "      <td>3.079129</td>\n",
       "      <td>0.195373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.152400</td>\n",
       "      <td>3.057187</td>\n",
       "      <td>0.231362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.145400</td>\n",
       "      <td>3.035934</td>\n",
       "      <td>0.246787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.065200</td>\n",
       "      <td>3.011708</td>\n",
       "      <td>0.257069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.070000</td>\n",
       "      <td>2.987786</td>\n",
       "      <td>0.293059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.048300</td>\n",
       "      <td>2.962295</td>\n",
       "      <td>0.308483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.048500</td>\n",
       "      <td>2.932249</td>\n",
       "      <td>0.339332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.965300</td>\n",
       "      <td>2.905476</td>\n",
       "      <td>0.341902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.976500</td>\n",
       "      <td>2.881428</td>\n",
       "      <td>0.336761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.905800</td>\n",
       "      <td>2.859384</td>\n",
       "      <td>0.344473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.970500</td>\n",
       "      <td>2.837116</td>\n",
       "      <td>0.354756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.900900</td>\n",
       "      <td>2.815300</td>\n",
       "      <td>0.372751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.934300</td>\n",
       "      <td>2.793713</td>\n",
       "      <td>0.388175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.873500</td>\n",
       "      <td>2.772080</td>\n",
       "      <td>0.424165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.881300</td>\n",
       "      <td>2.752194</td>\n",
       "      <td>0.439589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.918200</td>\n",
       "      <td>2.732584</td>\n",
       "      <td>0.439589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.773100</td>\n",
       "      <td>2.710604</td>\n",
       "      <td>0.462725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.839700</td>\n",
       "      <td>2.690330</td>\n",
       "      <td>0.462725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.757100</td>\n",
       "      <td>2.671359</td>\n",
       "      <td>0.470437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.825000</td>\n",
       "      <td>2.652935</td>\n",
       "      <td>0.473008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.831100</td>\n",
       "      <td>2.635199</td>\n",
       "      <td>0.493573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.736300</td>\n",
       "      <td>2.619091</td>\n",
       "      <td>0.501285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.750800</td>\n",
       "      <td>2.603116</td>\n",
       "      <td>0.506427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.683100</td>\n",
       "      <td>2.588164</td>\n",
       "      <td>0.511568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.810100</td>\n",
       "      <td>2.572908</td>\n",
       "      <td>0.519280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.711100</td>\n",
       "      <td>2.560322</td>\n",
       "      <td>0.521851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.644200</td>\n",
       "      <td>2.549293</td>\n",
       "      <td>0.534704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.648800</td>\n",
       "      <td>2.539256</td>\n",
       "      <td>0.532134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.643000</td>\n",
       "      <td>2.528520</td>\n",
       "      <td>0.539846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.666500</td>\n",
       "      <td>2.517352</td>\n",
       "      <td>0.532134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.651500</td>\n",
       "      <td>2.507651</td>\n",
       "      <td>0.529563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.620800</td>\n",
       "      <td>2.497861</td>\n",
       "      <td>0.534704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.703900</td>\n",
       "      <td>2.488559</td>\n",
       "      <td>0.534704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.539800</td>\n",
       "      <td>2.480695</td>\n",
       "      <td>0.550129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.652100</td>\n",
       "      <td>2.474181</td>\n",
       "      <td>0.552699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.705100</td>\n",
       "      <td>2.468524</td>\n",
       "      <td>0.552699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.594500</td>\n",
       "      <td>2.462726</td>\n",
       "      <td>0.552699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.545800</td>\n",
       "      <td>2.457588</td>\n",
       "      <td>0.552699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.624500</td>\n",
       "      <td>2.453286</td>\n",
       "      <td>0.557841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.616100</td>\n",
       "      <td>2.450115</td>\n",
       "      <td>0.555270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.411800</td>\n",
       "      <td>2.446818</td>\n",
       "      <td>0.560411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.615800</td>\n",
       "      <td>2.444327</td>\n",
       "      <td>0.562982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>2.539500</td>\n",
       "      <td>2.442778</td>\n",
       "      <td>0.562982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.546300</td>\n",
       "      <td>2.441863</td>\n",
       "      <td>0.562982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=585, training_loss=2.9015784663012902, metrics={'train_runtime': 66.6871, 'train_samples_per_second': 69.954, 'train_steps_per_second': 8.772, 'total_flos': 20767034735856.0, 'train_loss': 2.9015784663012902, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "efa96b1127d5636e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
