{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AI Workshop - Lab 2-2: Intent Classification\n",
    "\n",
    "In this lab, we’ll build a system to classify customer text messages into different categories (called **intents**) using a powerful type of AI model called a transformer. Transformers are a key technology behind tools like ChatGPT and other modern language systems.\n",
    "\n",
    "### Data Overview\n",
    "\n",
    "We’re working with a dataset of customer text messages that has already been labeled with their intent (e.g., \"Order Status\", \"Product Inquiry\", \"Account Help\"). The goal is to teach the model to recognize these patterns so it can classify new messages correctly.\n",
    "\n",
    "- **Number of Categories**: 27 different intents.\n",
    "\n",
    "### What You’ll Learn\n",
    "- **Transformers**: Get an introduction to these models and why they’re so powerful for language tasks.\n",
    "- **Model Evaluation**: Understand how to measure a model’s performance and interpret its predictions."
   ],
   "id": "89800ddaafaa2296"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -Uq datasets transformers accelerate evaluate sentencepiece",
   "id": "3d3c731f7713c24a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For this lab, it's essential that we have a GPU available to speed up training. On Google Colab, you can enable a GPU by going to **Runtime** > **Change runtime type** > **Hardware accelerator** > **GPU**.\n",
    "\n",
    "The following line of code will check if a GPU is available:"
   ],
   "id": "1bcffd8486a0820c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print('GPU is available!')\n",
    "else:\n",
    "    print('GPU is not available. Enable a GPU runtime in Colab under \"Runtime\" > \"Change runtime type\".')"
   ],
   "id": "967b287ee6c7830c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "Now that we’ve set up our environment and imported the necessary packages, let’s begin by loading our dataset.\n",
    "\n",
    "In this lab, we’ll work with a dataset of **customer text messages** that have been labeled with their **intent**. Each sample in the dataset includes a text message and a corresponding label indicating the intent behind the message (e.g., inquiry, complaint, order request). This dataset will allow us to build and evaluate models for intent classification.\n",
    "\n",
    "#### Steps:\n",
    "1. **Load the Dataset**:\n",
    "   - Use the `load_dataset` function from the `datasets` library to download and load the dataset.\n",
    "   - The dataset we’re using is hosted at `\"alexwaolson/customer-intents\"`.\n",
    "2. **Inspect the Dataset**:\n",
    "   - After loading, examine the training split (`intents['train']`) to understand its structure and the data it contains."
   ],
   "id": "b3a8dbd2fdcd2d6b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the customer intents dataset\n",
    "intents = load_dataset(\"alexwaolson/customer-intents\")\n",
    "\n",
    "# Display the training split\n",
    "pd.DataFrame(intents['train'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The dataset consists of two key columns:\n",
    "- **`message`**: Contains the text of the customer message.\n",
    "- **`label`**: Contains the intent category for each message.\n",
    "\n",
    "There are **27 possible intent categories** in this dataset. To understand the distribution of these categories, we can count the number of examples for each intent. This helps us determine whether the dataset is balanced (i.e., whether all categories have similar representation) or imbalanced (some categories have significantly more or fewer samples than others).\n",
    "\n",
    "Run the code below to calculate the distribution of intent labels:"
   ],
   "id": "9b2a9381462eaa7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the occurrences of each intent label in the training data\n",
    "label_counts = Counter(intents['train']['label'])\n",
    "print(f'Number of unique intents: {len(label_counts)}')\n",
    "\n",
    "# Plot the distribution of intent labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(label_counts.keys(), label_counts.values())\n",
    "plt.xlabel('Intent Label')\n",
    "plt.ylabel('Number of Examples')\n",
    "plt.title('Distribution of Intent Labels')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "id": "4b11c9f3d9801d3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Zero-Shot Learning\n",
    "\n",
    "One of the most powerful features of large language models is their ability to perform **zero-shot learning**. Unlike traditional models that require task-specific training, a zero-shot learning model can classify text based on its general understanding of language, even if it hasn’t been explicitly trained on that specific task.\n",
    "\n",
    "#### How It Works:\n",
    "- Instead of fine-tuning the model, you provide it with a **prompt** that describes the task and possible labels (e.g., \"What is the intent of this message?\").\n",
    "- The model uses its pre-trained knowledge to predict the most appropriate label.\n",
    "\n",
    "This approach leverages the model's extensive training on a wide variety of text, making it flexible for many tasks.\n",
    "\n",
    "#### Why Use Zero-Shot Learning?\n",
    "- **Quick Prototyping**: No need to preprocess or fine-tune the model for every new task.\n",
    "- **Versatility**: Works for tasks the model wasn’t explicitly trained on, as long as the task can be described in a prompt.\n",
    "\n",
    "#### Model Selection:\n",
    "For zero-shot classification, we’ll use the `flan-t5-large` model, which is well suited for this task due to its size and broad understanding of language. Since this model doesn’t require fine-tuning, we can focus on testing its performance directly."
   ],
   "id": "f4d4fa07abd72385"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Zero-Shot Intent Classification with Flan-T5\n",
    "\n",
    "We’ll now use the **Flan-T5 large** model to classify intents via zero-shot learning. This approach involves crafting a **prompt** that describes the task and provides the model with the possible labels. The model then uses its language understanding to predict the intent without task-specific training.\n",
    "\n",
    "#### Prompt Construction\n",
    "The prompt is key to zero-shot learning. For our task:\n",
    "1. The prompt begins by instructing the model to classify the intent of the message.\n",
    "2. It lists the available intent categories.\n",
    "3. Finally, it appends the message to classify."
   ],
   "id": "7dba3e9710c2523d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the prompt\n",
    "prompt = \"Classify the intent of the following message using these categories:\\n\"\n",
    "for label in label_counts.keys():\n",
    "    prompt += f\"- {label}\\n\"\n",
    "prompt += \"Message: \"\n",
    "\n",
    "print(prompt)"
   ],
   "id": "939de04f2e38e03d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "3029bea2fdfcee9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function for zero-shot classification\n",
    "def zero_shot_intent_classification(model, prompt, message):\n",
    "    # Combine the prompt and the message\n",
    "    input_text = prompt + message\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # Generate a prediction\n",
    "    output = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "    # Decode the prediction into text\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the function\n",
    "zero_shot_intent_classification(model, prompt, \"I need to cancel my order\")"
   ],
   "id": "980a2a232bc8e4a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Testing Zero-Shot Intent Classification\n",
    "\n",
    "You can now test the zero-shot classification capabilities of the `flan-t5-large` model on a subset of messages from the test set. This will provide a sense of how well the model performs without task-specific training."
   ],
   "id": "6526c7772ced2e9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for message in intents['test']['message'][25:35]:\n",
    "    print(f\"Message: {message}\")\n",
    "    print(f\"Predicted Intent: {zero_shot_intent_classification(model, prompt, message)}\")\n",
    "    print()"
   ],
   "id": "ac91494295609a37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Predicting Intent At Scale\n",
    "\n",
    "To evaluate the performance of the `flan-t5-large` zero-shot model on the entire test dataset, we’ll:\n",
    "1. **Generate Predictions**: Use the `zero_shot_intent_classification` function to predict intents for all test messages.\n",
    "2. **Compare Predictions**: Compare the zero-shot predictions to the true labels in the test set.\n",
    "3. **Examine mis-classified text**: Look at incorrectly classified examples to see if we can understand what went wrong."
   ],
   "id": "a3121167999eefa2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "zero_shot_predictions = [zero_shot_intent_classification(model, prompt, message) for message in tqdm(intents['test']['message'])]\n",
    "true_labels_text = intents['test']['label']"
   ],
   "id": "92119156ba498ead",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(true_labels_text, zero_shot_predictions)}')"
   ],
   "id": "78c8c23b2514924f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Incredibly, our accuracy using zero-shot learning is around **70%**, even without training on the categories first! Let's take a look at accuracy by category to see if there are any that the model struggles on. The **classification report** will break down the performance of the model by category, allowing us to understand if some categories are less well supported by the model than others. It provides us with the following information:\n",
    "\n",
    "- **Precision**: This measures the proportion of correctly predicted positive observations to the total predicted positives. High precision indicates that the model makes few false positive errors.\n",
    "\n",
    "  $$\n",
    "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "  $$\n",
    "\n",
    "  - **Example**: If the task is to classify emails as \"spam,\" a **true positive** is an email correctly classified as spam, while a **false positive** is a legitimate email incorrectly classified as spam.\n",
    "\n",
    "- **Recall**: This measures the proportion of correctly predicted positive observations to all the actual positives. High recall indicates that the model captures most of the true positive cases.\n",
    "\n",
    "  $$\n",
    "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "  $$\n",
    "\n",
    "  - **Example**: In the same email classification task, a **true positive** is an email correctly classified as spam, while a **false negative** is a spam email incorrectly classified as legitimate.\n",
    "\n",
    "- **F1 Score**: This is the harmonic mean of precision and recall, balancing the two metrics. A high F1 score indicates a good trade-off between precision and recall.\n",
    "\n",
    "  $$\n",
    "  \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  $$\n",
    "\n",
    "- **Support**: This refers to the number of actual occurrences of each category in the dataset. It helps us understand the distribution of the categories and whether any are underrepresented, which can impact performance metrics."
   ],
   "id": "e268670fc0f22457"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true_labels_text, zero_shot_predictions, zero_division=0))"
   ],
   "id": "d85e8b5003268666",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's now look at some mis-classified examples to see if we can understand why they were not classified correctly.",
   "id": "df372e69f79f9d77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display mis-classified examples\n",
    "misclassified_examples = [(message, true_label, pred) for message, true_label, pred in zip(intents['test']['message'], true_labels_text, zero_shot_predictions) if true_label != pred]\n",
    "pd.DataFrame(misclassified_examples, columns=['Message', 'True Label', 'Predicted Label'])"
   ],
   "id": "10f63b4d69a0e959",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Flagging Abuse\n",
    "\n",
    "One of the challenges in customer service is identifying and handling abusive messages. Even in this dataset there are examples where customers have used inappropriate language in their requests."
   ],
   "id": "70d633266db70103"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display abusive examples\n",
    "pd.DataFrame([(message, label) for message, label in zip(intents['test']['message'], true_labels_text) if 'damn' in message.lower()], columns=['Message', 'Label'])"
   ],
   "id": "9eef1c1909796d8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's say that we want to introduce a new task to classify messages as abusive or not. We can use the same zero-shot approach to classify messages as abusive or not abusive. The prompt will be similar to the previous one, but with the new task and labels.",
   "id": "af72110feb026910"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the prompt for abusive language classification\n",
    "abuse_prompt = \"Classify this message as abusive if it contains inappropriate language or not abusive if it does not.\\n\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "abuse_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "abuse_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function for zero-shot classification of abusive language\n",
    "def zero_shot_abuse_classification(model, prompt, message):\n",
    "    # Combine the prompt and the message\n",
    "    input_text = prompt + message\n",
    "    # Tokenize the input text\n",
    "    input_ids = abuse_tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # Generate a prediction\n",
    "    output = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "    # Decode the prediction into text\n",
    "    return abuse_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the function\n",
    "zero_shot_abuse_classification(abuse_model, abuse_prompt, \"I'm so damn frustrated with your service!\")"
   ],
   "id": "67f41026a4d9a6e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can use the model to flag messages as inappropriate or not inappropriate:",
   "id": "8b9144eb197a15ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Predict for samples that contain the word \"damn\"\n",
    "for message in intents['test']['message']:\n",
    "    if 'damn' in message.lower():\n",
    "        print(f\"Message: {message}\")\n",
    "        print(f\"Predicted Intent: {zero_shot_abuse_classification(abuse_model, abuse_prompt, message)}\")\n",
    "        print()"
   ],
   "id": "4ab40ba8c1e44cbe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Translation\n",
    "\n",
    "In some cases, it may be useful to translate customer messages into a different language. This can help customer service teams understand and respond to messages in languages they don't speak. Let's use the `Hugging Face` library to translate a sample message from English to French."
   ],
   "id": "3af47783324aa854"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "en_fr_translator = pipeline(\"translation_en_to_fr\")\n",
    "\n",
    "# Translate a sample message\n",
    "en_message = \"I need help with my order.\""
   ],
   "id": "cf41074817301920"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Translate the message\n",
    "fr_message = en_fr_translator(en_message)[0]['translation_text']\n",
    "print(fr_message)"
   ],
   "id": "1ca1f02fe053415f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Translate first 50 messages in the test set\n",
    "en_messages = intents['test']['message'][:50]\n",
    "fr_messages = en_fr_translator(en_messages)\n",
    "\n",
    "# Display the translations\n"
   ],
   "id": "8e19a682f2fb6f0f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
