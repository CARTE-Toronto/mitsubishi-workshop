{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AI Workshop - Lab 2-2: Intent Classification\n",
    "\n",
    "In this lab, we’ll build a system to classify customer text messages into different categories (called **intents**) using a powerful type of AI model called a transformer. Transformers are a key technology behind tools like ChatGPT and other modern language systems, but don’t worry if you’re new to them—we’ll break it down step by step.\n",
    "\n",
    "### Data Overview\n",
    "\n",
    "We’re working with a dataset of customer text messages that has already been labeled with their intent (e.g., \"Order Status\", \"Product Inquiry\", \"Account Help\"). The goal is to teach the model to recognize these patterns so it can classify new messages correctly.\n",
    "\n",
    "- **Dataset**:\n",
    "  - Provided as two files: one for training and one for testing.\n",
    "  - Training data is used to teach the model, and testing data is used to see how well it learned.\n",
    "- **Number of Categories**: 27 different intents.\n",
    "\n",
    "### What We’ll Do in This Lab\n",
    "1. **Load the Data**:\n",
    "   - Open and inspect the dataset to understand its structure.\n",
    "   - Check how many examples we have for each intent.\n",
    "2. **Prepare the Data**:\n",
    "   - Use a tool called a **tokenizer** to break down text messages into a format the model can understand.\n",
    "   - Convert the intent labels into numbers so the model can learn from them.\n",
    "3. **Use a Pre-Trained Model**:\n",
    "   - Start with an existing model called `T5-small` that already knows a lot about language.\n",
    "   - Customize (or fine-tune) it to focus on the intents in our dataset.\n",
    "4. **Train the Model**:\n",
    "   - Use the prepared data to train the model step by step.\n",
    "   - Measure how well it’s doing along the way.\n",
    "5. **Evaluate the Model**:\n",
    "   - Test the model on new data it hasn’t seen before.\n",
    "   - Check how accurate it is and where it might make mistakes.\n",
    "\n",
    "### What You’ll Learn\n",
    "- **Transformers**: Get an introduction to these models and why they’re so powerful for language tasks.\n",
    "- **Fine-Tuning**: Learn how to take a pre-trained model and adapt it to solve a specific problem.\n",
    "- **Model Evaluation**: Understand how to measure a model’s performance and interpret its predictions.\n",
    "\n",
    "### HuggingFace Libraries\n",
    "\n",
    "So far we have been working with Keras, a popular library for building neural networks. In this lab, we’ll use the HuggingFace libraries, which are designed specifically for working with transformers.\n",
    "\n",
    "The main HuggingFace library is called `transformers`, and it provides tools for working with pre-trained models, tokenizers, and training pipelines. We’ll also use `datasets` to load and process our data. `accelerate` and `evaluate` are additional libraries that help speed up training and evaluate models, respectively. Install them below:"
   ],
   "id": "89800ddaafaa2296"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:20:56.243917Z",
     "start_time": "2024-12-02T19:20:54.109389Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -Uq datasets transformers accelerate evaluate",
   "id": "3d3c731f7713c24a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For this lab, it's essential that we have a GPU available to speed up training. On Google Colab, you can enable a GPU by going to **Runtime** > **Change runtime type** > **Hardware accelerator** > **GPU**.\n",
    "\n",
    "The following line of code will check if a GPU is available:"
   ],
   "id": "1bcffd8486a0820c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:28:03.363531Z",
     "start_time": "2024-12-02T19:28:03.360417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print('GPU is available!')\n",
    "else:\n",
    "    print('GPU is not available. Enable a GPU runtime in Colab under \"Runtime\" > \"Change runtime type\".')"
   ],
   "id": "967b287ee6c7830c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Loading the Dataset\n",
    "\n",
    "Great! Now that we have our packages installed and imported, we can get going with loading the dataset.\n",
    "\n",
    "We will be working with a dataset of customer text messages that have been labeled with their intent. Let's load the dataset and inspect it to understand its structure."
   ],
   "id": "b3a8dbd2fdcd2d6b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-02T19:28:37.518397Z",
     "start_time": "2024-12-02T19:28:37.516169Z"
    }
   },
   "source": "from datasets import load_dataset",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:28:38.551600Z",
     "start_time": "2024-12-02T19:28:37.854767Z"
    }
   },
   "cell_type": "code",
   "source": "intents = load_dataset(\"alexwaolson/customer-intents\")",
   "id": "7d83bae93beee534",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:28:38.563065Z",
     "start_time": "2024-12-02T19:28:38.559700Z"
    }
   },
   "cell_type": "code",
   "source": "intents['train']",
   "id": "299346eb1c241001",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['message', 'label'],\n",
       "    num_rows: 1555\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you can see, the dataset is comprised of `message` and `label` columns. The `message` column contains the text of the customer message, and the `label` column contains the intent category. There are 27 possible intent categories in this dataset. We can count how many examples we have for each intent to see if the dataset is balanced.",
   "id": "9b2a9381462eaa7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:29:08.938933Z",
     "start_time": "2024-12-02T19:29:08.931003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(intents['train']['label'])"
   ],
   "id": "4b11c9f3d9801d3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'edit account': 65,\n",
       "         'delivery period': 65,\n",
       "         'get refund': 63,\n",
       "         'check payment methods': 62,\n",
       "         'change shipping address': 62,\n",
       "         'check cancellation fee': 62,\n",
       "         'check invoice': 61,\n",
       "         'payment issue': 60,\n",
       "         'set up shipping address': 59,\n",
       "         'create account': 58,\n",
       "         'track refund': 58,\n",
       "         'complaint': 58,\n",
       "         'contact customer service': 58,\n",
       "         'change order': 58,\n",
       "         'switch account': 57,\n",
       "         'cancel order': 57,\n",
       "         'get invoice': 56,\n",
       "         'track order': 56,\n",
       "         'newsletter subscription': 56,\n",
       "         'recover password': 55,\n",
       "         'delete account': 54,\n",
       "         'delivery options': 54,\n",
       "         'place order': 53,\n",
       "         'check refund policy': 53,\n",
       "         'contact human agent': 52,\n",
       "         'registration problems': 52,\n",
       "         'review': 51})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preparing the Data\n",
    "\n",
    "Now that we have our dataset loaded, we need to prepare it for training. This involves **tokenizing** the inputs and converting the labels into a format the model can understand.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Up until now we have been working with data that's easily converted into numbers (like images or tabular data). But with text, we need to do some extra work to convert it into a format the model can understand.\n",
    "\n",
    "The first step is to **tokenize** the text. Tokenization is the process of breaking down text into smaller parts called **tokens**. Tokens are typically words, but can also be **subwords** or **characters**. For example, the sentence \"Hello, how are you?\" might be tokenized into `['Hello', ',', 'how', 'are', 'you', '?']`.\n",
    "\n",
    "We are going to be working with a pre-trained model called `T5-small`. This model expects its input in a specific format, so we need to use its tokenizer to convert our text into tokens."
   ],
   "id": "88b3c3de6ae442f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:30:56.518251Z",
     "start_time": "2024-12-02T19:30:56.388002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load our tokenizer\n",
    "model_name = 't5-small'\n",
    "# The AutoTokenizer class will automatically select the correct tokenizer class for the model!\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "id": "c1d73e9530267d08",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:30:58.776488Z",
     "start_time": "2024-12-02T19:30:58.772099Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer('Hello, how are you?')",
   "id": "4e29ee94454c7eb7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [8774, 6, 149, 33, 25, 58, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's inspect the tokenizer a bit more closely to understand how it works. We can use the `show_tokenization` function below to see how a sentence gets tokenized by the model.\n",
    "\n",
    "**Task**: Run the `show_tokenization` function with a sentence of your choice to see how it gets tokenized."
   ],
   "id": "d31cbc545d9c61ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:31:29.669206Z",
     "start_time": "2024-12-02T19:31:29.664383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def show_tokenization(tokenizer, text):\n",
    "    print(f'Original text: {text}')\n",
    "    tokens = tokenizer(text, truncation=True)['input_ids']\n",
    "    for token in tokens:\n",
    "        print(f'{tokenizer.decode([token]):10} -> {token}')\n",
    "\n",
    "# Write any sentence and see how it gets tokenized by the model:\n",
    "show_tokenization(tokenizer, 'your sentence here')"
   ],
   "id": "33041b43d6be5ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: your sentence here\n",
      "your       -> 39\n",
      "sentence   -> 7142\n",
      "here       -> 270\n",
      "</s>       -> 1\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Hopefully you can see how the tokenizer works now! We also glossed over a term which you might spot later on too: the `attention_mask`. This is a sequence of 1s and 0s that tells the model which tokens to pay attention to and which to ignore. It's a crucial part of how transformers work, but we don't need to worry about it too much for now.\n",
    "\n",
    "### Padding\n",
    "\n",
    "Another important step in preparing the data is **padding**. When we tokenize text, we end up with sequences of different lengths. But the model expects inputs of the same length, so we need to pad the sequences to make them equal. This is done by adding a special token called `[PAD]` to the shorter sequences. The function below will handle this for us, as well as tokenize the full dataset."
   ],
   "id": "a7d6c2e291334223"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:32:25.002337Z",
     "start_time": "2024-12-02T19:32:24.997169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"message\"], truncation=True)"
   ],
   "id": "fffa6578628279e9",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:32:25.499123Z",
     "start_time": "2024-12-02T19:32:25.480027Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_intents = intents.map(preprocess_function, batched=True)",
   "id": "46a2ba13951d2fcc",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:32:26.304950Z",
     "start_time": "2024-12-02T19:32:26.300776Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_intents['train']",
   "id": "1c5fbcf9894aee67",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['message', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1555\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:32:27.923453Z",
     "start_time": "2024-12-02T19:32:27.921054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ],
   "id": "24864c6472656092",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:21:16.657956Z",
     "start_time": "2024-12-02T19:21:15.582767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ],
   "id": "bcb2ca52ef98dd4b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:21:16.668256Z",
     "start_time": "2024-12-02T19:21:16.664645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # Unpack the predictions and labels\n",
    "    predictions, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "\n",
    "    # Handle tuple predictions\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]  # Take the first element, assuming it's the logits\n",
    "\n",
    "    # Convert to NumPy array if necessary\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Compute class predictions\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Return computed metrics\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n"
   ],
   "id": "d2d1cb5dca0d2ea9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:21:16.768483Z",
     "start_time": "2024-12-02T19:21:16.752749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert labels to integers\n",
    "label2id = {label: i for i, label in enumerate(intents['train'].unique('label'))}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "def encode_label(example):\n",
    "    example['label'] = label2id[example['label']]\n",
    "    return example\n",
    "\n",
    "tokenized_intents = tokenized_intents.map(encode_label)"
   ],
   "id": "b76a3c69846fb5b6",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:21:16.850443Z",
     "start_time": "2024-12-02T19:21:16.844759Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_intents['train']",
   "id": "600bb1d54469c32b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['message', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1555\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:21:17.359622Z",
     "start_time": "2024-12-02T19:21:16.927419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"t5-small\",\n",
    "    num_labels=27,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ],
   "id": "af740a75def25f6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:21:51.510317Z",
     "start_time": "2024-12-02T19:21:50.405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    output_dir='output'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_intents['train'],\n",
    "    eval_dataset=tokenized_intents['test'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "473c1e3c017258a8",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:22:58.979131Z",
     "start_time": "2024-12-02T19:21:52.002197Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "e0d9f665319dec61",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='585' max='585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [585/585 01:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.375800</td>\n",
       "      <td>3.307129</td>\n",
       "      <td>0.061697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.347900</td>\n",
       "      <td>3.284916</td>\n",
       "      <td>0.074550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.296900</td>\n",
       "      <td>3.266773</td>\n",
       "      <td>0.079692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.334000</td>\n",
       "      <td>3.250174</td>\n",
       "      <td>0.100257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.345600</td>\n",
       "      <td>3.234811</td>\n",
       "      <td>0.125964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.271500</td>\n",
       "      <td>3.222997</td>\n",
       "      <td>0.125964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.282300</td>\n",
       "      <td>3.214721</td>\n",
       "      <td>0.120823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.232200</td>\n",
       "      <td>3.203785</td>\n",
       "      <td>0.118252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.250700</td>\n",
       "      <td>3.191883</td>\n",
       "      <td>0.113111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.231400</td>\n",
       "      <td>3.177467</td>\n",
       "      <td>0.118252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.200100</td>\n",
       "      <td>3.157994</td>\n",
       "      <td>0.164524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.149900</td>\n",
       "      <td>3.140149</td>\n",
       "      <td>0.167095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.211000</td>\n",
       "      <td>3.122959</td>\n",
       "      <td>0.177378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.203600</td>\n",
       "      <td>3.101764</td>\n",
       "      <td>0.190231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.208200</td>\n",
       "      <td>3.079129</td>\n",
       "      <td>0.195373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.152400</td>\n",
       "      <td>3.057187</td>\n",
       "      <td>0.231362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.145400</td>\n",
       "      <td>3.035934</td>\n",
       "      <td>0.246787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.065200</td>\n",
       "      <td>3.011708</td>\n",
       "      <td>0.257069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.070000</td>\n",
       "      <td>2.987786</td>\n",
       "      <td>0.293059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.048300</td>\n",
       "      <td>2.962295</td>\n",
       "      <td>0.308483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.048500</td>\n",
       "      <td>2.932249</td>\n",
       "      <td>0.339332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.965300</td>\n",
       "      <td>2.905476</td>\n",
       "      <td>0.341902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.976500</td>\n",
       "      <td>2.881428</td>\n",
       "      <td>0.336761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.905800</td>\n",
       "      <td>2.859384</td>\n",
       "      <td>0.344473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.970500</td>\n",
       "      <td>2.837116</td>\n",
       "      <td>0.354756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.900900</td>\n",
       "      <td>2.815300</td>\n",
       "      <td>0.372751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.934300</td>\n",
       "      <td>2.793713</td>\n",
       "      <td>0.388175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.873500</td>\n",
       "      <td>2.772080</td>\n",
       "      <td>0.424165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.881300</td>\n",
       "      <td>2.752194</td>\n",
       "      <td>0.439589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.918200</td>\n",
       "      <td>2.732584</td>\n",
       "      <td>0.439589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.773100</td>\n",
       "      <td>2.710604</td>\n",
       "      <td>0.462725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.839700</td>\n",
       "      <td>2.690330</td>\n",
       "      <td>0.462725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.757100</td>\n",
       "      <td>2.671359</td>\n",
       "      <td>0.470437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.825000</td>\n",
       "      <td>2.652935</td>\n",
       "      <td>0.473008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.831100</td>\n",
       "      <td>2.635199</td>\n",
       "      <td>0.493573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.736300</td>\n",
       "      <td>2.619091</td>\n",
       "      <td>0.501285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.750800</td>\n",
       "      <td>2.603116</td>\n",
       "      <td>0.506427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.683100</td>\n",
       "      <td>2.588164</td>\n",
       "      <td>0.511568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.810100</td>\n",
       "      <td>2.572908</td>\n",
       "      <td>0.519280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.711100</td>\n",
       "      <td>2.560322</td>\n",
       "      <td>0.521851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.644200</td>\n",
       "      <td>2.549293</td>\n",
       "      <td>0.534704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.648800</td>\n",
       "      <td>2.539256</td>\n",
       "      <td>0.532134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.643000</td>\n",
       "      <td>2.528520</td>\n",
       "      <td>0.539846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.666500</td>\n",
       "      <td>2.517352</td>\n",
       "      <td>0.532134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.651500</td>\n",
       "      <td>2.507651</td>\n",
       "      <td>0.529563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.620800</td>\n",
       "      <td>2.497861</td>\n",
       "      <td>0.534704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.703900</td>\n",
       "      <td>2.488559</td>\n",
       "      <td>0.534704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.539800</td>\n",
       "      <td>2.480695</td>\n",
       "      <td>0.550129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.652100</td>\n",
       "      <td>2.474181</td>\n",
       "      <td>0.552699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.705100</td>\n",
       "      <td>2.468524</td>\n",
       "      <td>0.552699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.594500</td>\n",
       "      <td>2.462726</td>\n",
       "      <td>0.552699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.545800</td>\n",
       "      <td>2.457588</td>\n",
       "      <td>0.552699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.624500</td>\n",
       "      <td>2.453286</td>\n",
       "      <td>0.557841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.616100</td>\n",
       "      <td>2.450115</td>\n",
       "      <td>0.555270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.411800</td>\n",
       "      <td>2.446818</td>\n",
       "      <td>0.560411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.615800</td>\n",
       "      <td>2.444327</td>\n",
       "      <td>0.562982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>2.539500</td>\n",
       "      <td>2.442778</td>\n",
       "      <td>0.562982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.546300</td>\n",
       "      <td>2.441863</td>\n",
       "      <td>0.562982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=585, training_loss=2.9015784663012902, metrics={'train_runtime': 66.6871, 'train_samples_per_second': 69.954, 'train_steps_per_second': 8.772, 'total_flos': 20767034735856.0, 'train_loss': 2.9015784663012902, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "efa96b1127d5636e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
